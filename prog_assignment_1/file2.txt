1 2 3 4 5 6 7 8 9101 2 3 4 51 2 3 4 5 6 7 86.006 Intro to Algorithms Recitation 2 September 11, 2013Document Distancedocdist1.py is a straightforward solution to the document distance problem, and docdist{2-8}.py show algorithmic optimizations that improve the running time.We start out by understanding the structure of the straightforward implementation, and then we’ll look at the changes in each of the successive versions.docdist1We first look at main to get a high-level view of what’s going on in the program.def main():if len(sys.argv) != 3:print "Usage: docdist1.py filename_1 filename_2" else:filename_1 = sys.argv[1]filename_2 = sys.argv[2]document_vector_1 = word_frequencies_for_file(filename_1) document_vector_2 = word_frequencies_for_file(filename_2)distance = vector_angle(document_vector_1,document_vector_2)print "The distance between the documents is: %0.6f (radians)"%distanceThemethodprocessesthecommand-linearguments,andcallsword frequencies for file for each document, then calls vector angle on the resulting lists. How do these methods matchupwiththethreeoperationsoutlinedinlecture?Itseemslikeword frequencies for file is responsible for operation 1 (split each document into words) and operation 2 (count word fre- quencies),andthenvector angleisresponsibleforoperation3(computedotproduct).Nextup,we’lltakealookatword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingThe method first calls read file, which returns a list of lines in the input file. We’ll omit the method code, because it is not particularly interesting, and we’ll assume that read file’s running time is proportional to the size of the input file. The input from read line is given to get words from line list, which computes operation 1 (split each document into words).Afterthat,count frequencyturnsthelistofwordsintoadocumentvector(operation 2).def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list = word_list + words_in_line return word_listdef get_words_from_string(line):
 91011121314151617181920212223elif len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word) character_list = []if len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word)return word_list1 2 3 4 5 6 7 8 9101 2 3get words from string takes one line in the input file and breaks it up into a list of words. We iterate through the for loop O(k) times where k is the length of the line. All of the operations within the loop take constant time to run, with the exception of join. Although the join operator takes O(c) time on an input of c characters, each character in the line is joined only once.Therefore,get words from stringhasanoverallruntimeofO(k).get words from line list calls get words from string for each line andcombines the lists into one big list. Line 5 looks innocent but is a big performance killer, becauseusing + to combine W lists of length k is O(W2 ). kkThe output of get words from line list is a list of words, like [’a’, ’cat’,’in’, ’a’, ’bag’].word frequencies from filepassesthisoutputtocount frequency, which turns it into a document vector that counts the number of occurrences of each word, and lookslike[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]].def count_frequency(word_list): L=[]for new_word in word_list: for entry in L:if new_word == entry[0]: entry[1] = entry[1] + 1 breakelse: L.append([new_word,1])return LThe implementation above builds the document vector by takes each word in the input list and looking it up in the list representing the under-construction document vector. In the worst case of a document with all different words, this takes O(W 2 × l) time, where W is the number of words in the document, and l is the average word length.count frequency is the last function call in word frequencies for file. Next up,maincallsvector angle,whichperformsoperation3,computingthedistancemetric.def vector_angle(L1,L2):numerator = inner_product(L1,L2)denominator = math.sqrt(inner_product(L1,L1)*inner_product(L2,L2))6.006 Intro to Algorithmsword_list = [] character_list = [] for c in line:if c.isalnum(): character_list.append(c)Recitation 2September 11, 2013
41 2 3 4 5 6 76.006 Intro to Algorithms Recitation 2 September 11, 2013return math.acos(numerator/denominator)The method is a somewhat straightforward implementation of the distance metric􏰀 L1 · L2 􏰁 􏰄 L1 · L2 􏰅 arccos |L1||L2| = arccos 􏰇(L1 · L1)(L2 · L2),anddelegatestoinner productforthehardworkofcomputingcrossproducts.def inner_product(L1,L2): sum = 0.0for word1, count1 in L1:for word2, count2 in L2: if word1 == word2:sum += count1 * count2 return suminner product is a straightforward inner-product implementation that checks each each word in the first list against the entire second list. The nested loops at lines 3 and 4 give the algorithm its running time of Θ(L1L2), where L1 and L2 are the lengths of the documents’ vectors (the number of unique words in each document).docdist1 Performance ScorecardWe assume that k (number of words per line) is a constant, because the documents will need to fit on screens or paper sheets with a finite length. W (the number of words in a document) is ≥ L (the number of unique words in a document). L21 + L2 + L1L2 = O(L21 + L2) because L21 + L2 ≥ L1L2. Proof (assuming L1, L2 ≥ 0):MethodTimeget words from line list count frequencyword frequencies for fileO(W2 ) = O(W2) kO(WL) O(W2)inner productvector angleO(L1L2) O(L1L2 + L21 + L2) = O(L21 + L2)mainO ( W 12 + W 2 2 )docdist2(L1−L2)2 ≥0 L21+L2−2L1L2 ≥ 0L21+L2 ≥ 2L1L2 L 21 + L 2 2 ≥ L 1 L 2The document distance code invokes the Python profiler to identify the code that takes up the most CPU time. This ensures that we get the biggest returns on our optimization efforts.
1 2 36.006 Intro to Algorithms Recitation 2 September 11, 2013if __name__ == "__main__": import cProfile cProfile.run("main()")You can profile existing programs without changing them by adding -m cProfile -s Time to Python’s command line. For example, the command below will run and profile program.py.  python -m cProfile -s time program.pyA sample profiler output is shown below.123456 ncalls tottime 7 1 0.001 8 2 0.000 9 3 6.25510 1 0.000 11 1 0.004 12 2 0.000 13 2 16.995 14 22663 0.793 15 2 11.205 16 325087 0.034 17 1 0.000 18 1 0.000 19 1241849 0.145 20 1 0.000 21 1300248 0.112 22 232140 0.054 23 232140 0.035 24 2 0.003 25 2 0.000The distance between the documents is: 0.574160 (radians) 3354148 function calls in 35.636 CPU secondsOrdered by: standard namepercall  cumtime  0.001   35.636  0.000   29.376  2.085    6.255  0.000    6.255  0.004   35.635  0.000    0.003  8.498   18.167  0.000    1.172  5.602   11.206  0.000    0.034  0.000    0.000  0.000    0.000  0.000    0.145  0.000    0.000  0.000    0.112  0.000    0.0540.000 0.0350.001 0.0030.000 0.000percall filename:lineno(function) 35.636 <string>:1(<module>) 14.688 docdist1.py:100(word_frequencies_for_file)  2.085 docdist1.py:110(inner_product)  6.255 docdist1.py:125(vector_angle) 35.635 docdist1.py:135(main)  0.001 docdist1.py:37(read_file)  9.083 docdist1.py:49(get_words_from_line_list)  0.000 docdist1.py:61(get_words_from_string)  5.603 docdist1.py:86(count_frequency)  0.000 {len}  0.000 {math.acos}  0.000 {math.sqrt}  0.000 {method ’append’ of ’list’ objects}  0.000 {method ’disable’ of ’_lsprof.Profiler’ objec  0.000 {method ’isalnum’ of ’str’ objects}  0.000 {method ’join’ of ’str’ objects}  0.000 {method ’lower’ of ’str’ objects}  0.001 {method ’readlines’ of ’file’ objects}  0.000 {open}1 2 3 4 5 6Theprofileroutputfordocdist1showsthatthebiggesttimedrainisget words from line list. The problem is that when the + operator is used to concatenate lists, it needs to create a new listand copy the elements of both its operands. Replacing + with extend yields a 30% runtime improvement.def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list.extend(words_in_line) return word_listextend adds all the elements of an m-element list to an n-element list in Θ(1+m), as opposed to +, which needs to create a new list, and therefore takes Θ(1 + n + m) time. So concatenatingkW lists of k elements takes 􏰆W k = Θ(W) time.k
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist2 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(WL) O(WL)inner productvector angleO(L1L2) O ( L 21 + L 2 2 )mainO(W1L1 + W2L2)1 2 3 4 5 61 2 3 4 5 6 7 8 91 2 3 4 5 6 7docdist3Profilingdocdist2pointstocount frequencyandinner productasgoodtargetsforop- timizations. We’ll speed up inner product by switching to a fast algorithm. However, the algorithmassumesthatthewordsinthedocumentvectorsaresorted.Forexample,[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]]needstobecome[[’a’, 2], [’bag’, 1], [’cat’, 1], [’in’, 1]].Firstoff,weaddasteptoword frequencies for filethatsortsthedocumentvector producedbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) insertion_sort(freq_mapping)return freq_mappingThenweimplementinsertion sortusingthealgorithminthetextbook.def insertion_sort(A): for j in range(len(A)):key = A[j]i = j-1while i>-1 and A[i]>key:A[i+1] = A[i]      i = i-1    A[i+1] = keyreturn AInsertion sort runs in O(L2) time, where L is the length of the document vector to be sorted.Finally,inner productisre-implementedusingasimilaralgorithmtothemergingstepin Merge Sort.def inner_product(L1,L2): sum = 0.0i=0j=0while i<len(L1) and j<len(L2):# L1[i:] and L2[j:] yet to be processed if L1[i][0] == L2[j][0]:
8 91011121314151617186.006 Intro to Algorithms Recitation 2 September 11, 2013      # both vectors have this word      sum += L1[i][1] * L2[j][1]      i += 1      j += 1elif L1[i][0] < L2[j][0]:# word L1[i][0] is in L1 but not L2 i += 1else:# word L2[j][0] is in L2 but not L1 j += 1return sumThe new implementation runs in Θ(L1 + L2), where L1 and L2 are the lengths of the two document vectors. We observe that the running time for inner product (and therefore for vector angle)isasymptoticallyoptimal,becauseanyalgorithmthatcomputestheinnerprod- uct will have to read the two vectors, and reading will take Ω(L1 + L2) time.docdist3 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(WL)O(L2) O(WL + L2) = O(WL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1L1 + W2L2)1 2 3 4 5 6 7 8docdist4The next iteration addresses count frequency, which is the biggest time consumer at the moment.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return D.items()The new implementation uses Python dictionaries. The dictionaries are implemented using hash tables, which will be presented in a future lecture. The salient feature of hash tables is that inserting an element using dictionary[key] = value and looking up an element using dictionary[key] both run in O(1) expected time.Instead of storing the document vector under construction in a list, the new implementation uses a dictionary. The keys are the words in the document, and the value are the number of times
6.006 Intro to Algorithms Recitation 2 September 11, 2013 each word appears in the document. Since both insertion (line 5) and lookup (line 7) take O(1)time, building a document vector out of W words takes O(W ) time. docdist4 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(W)O(L2) O(W + L2) = O(L2)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L21 +L2)1 2 3 4 5 6 7docdist5Thisiterationsimplifiesget words from stringthatbreaksuplinesintowords.Firstoff, the standard library function string.translate is used for converting uppercase characters to lowercase, and for converting punctuation to spaces. Second, the split method on strings is used to break up a line into words.translation_table = string.maketrans(string.punctuation+string.uppercase,                   " "*len(string.punctuation)+string.lowercase)def get_words_from_string(line):line = line.translate(translation_table) word_list = line.split()return word_listThe main benefit of this change is that 23 lines of code are replaced with 5 lines of code. This makes the implementation easier to analyze. A side benefit is that many functions in the Python standard library are implemented in directly in C (a low-level programming language that is very close to machine code), which gives them better performance. Although the running time is asymptotically the same, the hidden constants are much better for the C code than for our Python code presented in docdist1.docdist5 Performance ScorecardIdentical to the docdist4 scorecard.docdist6Nowthatallthedistractionsareoutoftheway,it’stimetotackleinsertion sort,whichis takes up the most CPU time, by far, in the profiler output for docdist5.The advantages of insertion sort are that it sorts in place, and it is simple to implement. However, its worst-case running time is O(N2) for an N-element array. We’ll replace insertion
1 2 3 4 5 61 2 3 4 5 6 7 8 9101112131415161718192021222324256.006 Intro to Algorithms Recitation 2 September 11, 2013 sort with a better algorithm, merge sort. Merge sort is not in place, so we’ll need to modifyword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) freq_mapping = merge_sort(freq_mapping)return freq_mappingThemerge sortimplementationcloselyfollowsthepseudocodeinthetextbook.def merge_sort(A): n = len(A)if n==1:return Amid = n//2L = merge_sort(A[:mid]) R = merge_sort(A[mid:]) return merge(L,R)def merge(L,R): i=0j=0answer = []while i<len(L) and j<len(R):if L[i]<R[j]: answer.append(L[i]) i += 1else: answer.append(R[j]) j += 1if i<len(L): answer.extend(L[i:])if j<len(R): answer.extend(R[j:])return answerThe textbook proves that merge sort runs in Θ(n log n) time. You should apply your knowledge of the Python cost model to convince yourself that the implementation above also runs in Θ(n log n) time.
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist6 Performance ScorecardMethodTimeget words from line list count frequencymerge sortword frequencies for fileO(W)O(W) O(L log L) O(W +LlogL)=O(LlogL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L1 logL1 +L2 logL2)1 2 3 4 5 6 7 81 2 3 4 51 2 3 4 5 6docdist7Switching to merge sort improved the running time dramatically. However, if we look at docdist6’s profiler output, we notice that merge is the function with the biggest runtime footprint. Merge sort’s performance in practice is great, so it seems that the only way to make our code faster is to get rid of sorting altogether.This iteration switches away from the sorted list representation of document vectors, and in- steadusesthePythondictionaryrepresentationthatwasintroducedindocdist4.count frequency already used that representation internally, so we only need to remove the code that converted the Python dictionary to a list.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return DThis method still takes O(W ) time to process a W -word document.word frequencies for file does not call merge sort anymore, and instead re- turnsthedictionarybuiltbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingNextup,inner productmakesusesdictionarylookupsinsteadofmergingsortedlists.def inner_product(D1,D2): sum = 0.0for key in D1:if key in D2:sum += D1[key] * D2[key]return sum
6.006 Intro to Algorithms Recitation 2 September 11, 2013The logic is quite similar to the straightforward inner product in docdist1. Each word in the first document vector is looked up in the second document vector. However, because the document vectors are dictionaries, each takes O(1) time, and inner product runs in O(L1) time, where L1 is the length of the first document’s vector.docdist7 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 910docdist8At this point, all the algorithms in our solution are asymptotically optimal. We can easily prove this, by noting that each of the 3 main operations runs in time proportional to its input size, and each operation needs to read all its input to produce its output. However, there is still room for optimizing and simplifying the code.There is no reason to read each document line by line, and then break up each line into words. The last iteration processes reads each document into one large string, and breaks up the entire document into words at once.Firstoff,read fileismodifiedtoreturnasinglestring,insteadofanarrayofstrings.Then,get words from line listisrenamedtoget words from file,andissimplified, becauseitdoesn’tneedtoiterateoveralistoflinesanymore.Last,word frequencies for file is updated to reflect the renaming.def get_words_from_string(string):string = string.translate(translation_table) word_list = string.split()return word_listdef word_frequencies_for_file(filename): text = read_file(filename)word_list = get_words_from_string(text) freq_mapping = count_frequency(word_list) return freq_mapping
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist8 Performance ScorecardMethodTimeget words from textcount frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 9101 2 3 4 51 2 3 4 5 6 7 86.006 Intro to Algorithms Recitation 2 September 11, 2013Document Distancedocdist1.py is a straightforward solution to the document distance problem, and docdist{2-8}.py show algorithmic optimizations that improve the running time.We start out by understanding the structure of the straightforward implementation, and then we’ll look at the changes in each of the successive versions.docdist1We first look at main to get a high-level view of what’s going on in the program.def main():if len(sys.argv) != 3:print "Usage: docdist1.py filename_1 filename_2" else:filename_1 = sys.argv[1]filename_2 = sys.argv[2]document_vector_1 = word_frequencies_for_file(filename_1) document_vector_2 = word_frequencies_for_file(filename_2)distance = vector_angle(document_vector_1,document_vector_2)print "The distance between the documents is: %0.6f (radians)"%distanceThemethodprocessesthecommand-linearguments,andcallsword frequencies for file for each document, then calls vector angle on the resulting lists. How do these methods matchupwiththethreeoperationsoutlinedinlecture?Itseemslikeword frequencies for file is responsible for operation 1 (split each document into words) and operation 2 (count word fre- quencies),andthenvector angleisresponsibleforoperation3(computedotproduct).Nextup,we’lltakealookatword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingThe method first calls read file, which returns a list of lines in the input file. We’ll omit the method code, because it is not particularly interesting, and we’ll assume that read file’s running time is proportional to the size of the input file. The input from read line is given to get words from line list, which computes operation 1 (split each document into words).Afterthat,count frequencyturnsthelistofwordsintoadocumentvector(operation 2).def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list = word_list + words_in_line return word_listdef get_words_from_string(line):
 91011121314151617181920212223elif len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word) character_list = []if len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word)return word_list1 2 3 4 5 6 7 8 9101 2 3get words from string takes one line in the input file and breaks it up into a list of words. We iterate through the for loop O(k) times where k is the length of the line. All of the operations within the loop take constant time to run, with the exception of join. Although the join operator takes O(c) time on an input of c characters, each character in the line is joined only once.Therefore,get words from stringhasanoverallruntimeofO(k).get words from line list calls get words from string for each line andcombines the lists into one big list. Line 5 looks innocent but is a big performance killer, becauseusing + to combine W lists of length k is O(W2 ). kkThe output of get words from line list is a list of words, like [’a’, ’cat’,’in’, ’a’, ’bag’].word frequencies from filepassesthisoutputtocount frequency, which turns it into a document vector that counts the number of occurrences of each word, and lookslike[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]].def count_frequency(word_list): L=[]for new_word in word_list: for entry in L:if new_word == entry[0]: entry[1] = entry[1] + 1 breakelse: L.append([new_word,1])return LThe implementation above builds the document vector by takes each word in the input list and looking it up in the list representing the under-construction document vector. In the worst case of a document with all different words, this takes O(W 2 × l) time, where W is the number of words in the document, and l is the average word length.count frequency is the last function call in word frequencies for file. Next up,maincallsvector angle,whichperformsoperation3,computingthedistancemetric.def vector_angle(L1,L2):numerator = inner_product(L1,L2)denominator = math.sqrt(inner_product(L1,L1)*inner_product(L2,L2))6.006 Intro to Algorithmsword_list = [] character_list = [] for c in line:if c.isalnum(): character_list.append(c)Recitation 2September 11, 2013
41 2 3 4 5 6 76.006 Intro to Algorithms Recitation 2 September 11, 2013return math.acos(numerator/denominator)The method is a somewhat straightforward implementation of the distance metric􏰀 L1 · L2 􏰁 􏰄 L1 · L2 􏰅 arccos |L1||L2| = arccos 􏰇(L1 · L1)(L2 · L2),anddelegatestoinner productforthehardworkofcomputingcrossproducts.def inner_product(L1,L2): sum = 0.0for word1, count1 in L1:for word2, count2 in L2: if word1 == word2:sum += count1 * count2 return suminner product is a straightforward inner-product implementation that checks each each word in the first list against the entire second list. The nested loops at lines 3 and 4 give the algorithm its running time of Θ(L1L2), where L1 and L2 are the lengths of the documents’ vectors (the number of unique words in each document).docdist1 Performance ScorecardWe assume that k (number of words per line) is a constant, because the documents will need to fit on screens or paper sheets with a finite length. W (the number of words in a document) is ≥ L (the number of unique words in a document). L21 + L2 + L1L2 = O(L21 + L2) because L21 + L2 ≥ L1L2. Proof (assuming L1, L2 ≥ 0):MethodTimeget words from line list count frequencyword frequencies for fileO(W2 ) = O(W2) kO(WL) O(W2)inner productvector angleO(L1L2) O(L1L2 + L21 + L2) = O(L21 + L2)mainO ( W 12 + W 2 2 )docdist2(L1−L2)2 ≥0 L21+L2−2L1L2 ≥ 0L21+L2 ≥ 2L1L2 L 21 + L 2 2 ≥ L 1 L 2The document distance code invokes the Python profiler to identify the code that takes up the most CPU time. This ensures that we get the biggest returns on our optimization efforts.
1 2 36.006 Intro to Algorithms Recitation 2 September 11, 2013if __name__ == "__main__": import cProfile cProfile.run("main()")You can profile existing programs without changing them by adding -m cProfile -s Time to Python’s command line. For example, the command below will run and profile program.py.  python -m cProfile -s time program.pyA sample profiler output is shown below.123456 ncalls tottime 7 1 0.001 8 2 0.000 9 3 6.25510 1 0.000 11 1 0.004 12 2 0.000 13 2 16.995 14 22663 0.793 15 2 11.205 16 325087 0.034 17 1 0.000 18 1 0.000 19 1241849 0.145 20 1 0.000 21 1300248 0.112 22 232140 0.054 23 232140 0.035 24 2 0.003 25 2 0.000The distance between the documents is: 0.574160 (radians) 3354148 function calls in 35.636 CPU secondsOrdered by: standard namepercall  cumtime  0.001   35.636  0.000   29.376  2.085    6.255  0.000    6.255  0.004   35.635  0.000    0.003  8.498   18.167  0.000    1.172  5.602   11.206  0.000    0.034  0.000    0.000  0.000    0.000  0.000    0.145  0.000    0.000  0.000    0.112  0.000    0.0540.000 0.0350.001 0.0030.000 0.000percall filename:lineno(function) 35.636 <string>:1(<module>) 14.688 docdist1.py:100(word_frequencies_for_file)  2.085 docdist1.py:110(inner_product)  6.255 docdist1.py:125(vector_angle) 35.635 docdist1.py:135(main)  0.001 docdist1.py:37(read_file)  9.083 docdist1.py:49(get_words_from_line_list)  0.000 docdist1.py:61(get_words_from_string)  5.603 docdist1.py:86(count_frequency)  0.000 {len}  0.000 {math.acos}  0.000 {math.sqrt}  0.000 {method ’append’ of ’list’ objects}  0.000 {method ’disable’ of ’_lsprof.Profiler’ objec  0.000 {method ’isalnum’ of ’str’ objects}  0.000 {method ’join’ of ’str’ objects}  0.000 {method ’lower’ of ’str’ objects}  0.001 {method ’readlines’ of ’file’ objects}  0.000 {open}1 2 3 4 5 6Theprofileroutputfordocdist1showsthatthebiggesttimedrainisget words from line list. The problem is that when the + operator is used to concatenate lists, it needs to create a new listand copy the elements of both its operands. Replacing + with extend yields a 30% runtime improvement.def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list.extend(words_in_line) return word_listextend adds all the elements of an m-element list to an n-element list in Θ(1+m), as opposed to +, which needs to create a new list, and therefore takes Θ(1 + n + m) time. So concatenatingkW lists of k elements takes 􏰆W k = Θ(W) time.k
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist2 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(WL) O(WL)inner productvector angleO(L1L2) O ( L 21 + L 2 2 )mainO(W1L1 + W2L2)1 2 3 4 5 61 2 3 4 5 6 7 8 91 2 3 4 5 6 7docdist3Profilingdocdist2pointstocount frequencyandinner productasgoodtargetsforop- timizations. We’ll speed up inner product by switching to a fast algorithm. However, the algorithmassumesthatthewordsinthedocumentvectorsaresorted.Forexample,[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]]needstobecome[[’a’, 2], [’bag’, 1], [’cat’, 1], [’in’, 1]].Firstoff,weaddasteptoword frequencies for filethatsortsthedocumentvector producedbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) insertion_sort(freq_mapping)return freq_mappingThenweimplementinsertion sortusingthealgorithminthetextbook.def insertion_sort(A): for j in range(len(A)):key = A[j]i = j-1while i>-1 and A[i]>key:A[i+1] = A[i]      i = i-1    A[i+1] = keyreturn AInsertion sort runs in O(L2) time, where L is the length of the document vector to be sorted.Finally,inner productisre-implementedusingasimilaralgorithmtothemergingstepin Merge Sort.def inner_product(L1,L2): sum = 0.0i=0j=0while i<len(L1) and j<len(L2):# L1[i:] and L2[j:] yet to be processed if L1[i][0] == L2[j][0]:
8 91011121314151617186.006 Intro to Algorithms Recitation 2 September 11, 2013      # both vectors have this word      sum += L1[i][1] * L2[j][1]      i += 1      j += 1elif L1[i][0] < L2[j][0]:# word L1[i][0] is in L1 but not L2 i += 1else:# word L2[j][0] is in L2 but not L1 j += 1return sumThe new implementation runs in Θ(L1 + L2), where L1 and L2 are the lengths of the two document vectors. We observe that the running time for inner product (and therefore for vector angle)isasymptoticallyoptimal,becauseanyalgorithmthatcomputestheinnerprod- uct will have to read the two vectors, and reading will take Ω(L1 + L2) time.docdist3 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(WL)O(L2) O(WL + L2) = O(WL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1L1 + W2L2)1 2 3 4 5 6 7 8docdist4The next iteration addresses count frequency, which is the biggest time consumer at the moment.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return D.items()The new implementation uses Python dictionaries. The dictionaries are implemented using hash tables, which will be presented in a future lecture. The salient feature of hash tables is that inserting an element using dictionary[key] = value and looking up an element using dictionary[key] both run in O(1) expected time.Instead of storing the document vector under construction in a list, the new implementation uses a dictionary. The keys are the words in the document, and the value are the number of times
6.006 Intro to Algorithms Recitation 2 September 11, 2013 each word appears in the document. Since both insertion (line 5) and lookup (line 7) take O(1)time, building a document vector out of W words takes O(W ) time. docdist4 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(W)O(L2) O(W + L2) = O(L2)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L21 +L2)1 2 3 4 5 6 7docdist5Thisiterationsimplifiesget words from stringthatbreaksuplinesintowords.Firstoff, the standard library function string.translate is used for converting uppercase characters to lowercase, and for converting punctuation to spaces. Second, the split method on strings is used to break up a line into words.translation_table = string.maketrans(string.punctuation+string.uppercase,                   " "*len(string.punctuation)+string.lowercase)def get_words_from_string(line):line = line.translate(translation_table) word_list = line.split()return word_listThe main benefit of this change is that 23 lines of code are replaced with 5 lines of code. This makes the implementation easier to analyze. A side benefit is that many functions in the Python standard library are implemented in directly in C (a low-level programming language that is very close to machine code), which gives them better performance. Although the running time is asymptotically the same, the hidden constants are much better for the C code than for our Python code presented in docdist1.docdist5 Performance ScorecardIdentical to the docdist4 scorecard.docdist6Nowthatallthedistractionsareoutoftheway,it’stimetotackleinsertion sort,whichis takes up the most CPU time, by far, in the profiler output for docdist5.The advantages of insertion sort are that it sorts in place, and it is simple to implement. However, its worst-case running time is O(N2) for an N-element array. We’ll replace insertion
1 2 3 4 5 61 2 3 4 5 6 7 8 9101112131415161718192021222324256.006 Intro to Algorithms Recitation 2 September 11, 2013 sort with a better algorithm, merge sort. Merge sort is not in place, so we’ll need to modifyword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) freq_mapping = merge_sort(freq_mapping)return freq_mappingThemerge sortimplementationcloselyfollowsthepseudocodeinthetextbook.def merge_sort(A): n = len(A)if n==1:return Amid = n//2L = merge_sort(A[:mid]) R = merge_sort(A[mid:]) return merge(L,R)def merge(L,R): i=0j=0answer = []while i<len(L) and j<len(R):if L[i]<R[j]: answer.append(L[i]) i += 1else: answer.append(R[j]) j += 1if i<len(L): answer.extend(L[i:])if j<len(R): answer.extend(R[j:])return answerThe textbook proves that merge sort runs in Θ(n log n) time. You should apply your knowledge of the Python cost model to convince yourself that the implementation above also runs in Θ(n log n) time.
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist6 Performance ScorecardMethodTimeget words from line list count frequencymerge sortword frequencies for fileO(W)O(W) O(L log L) O(W +LlogL)=O(LlogL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L1 logL1 +L2 logL2)1 2 3 4 5 6 7 81 2 3 4 51 2 3 4 5 6docdist7Switching to merge sort improved the running time dramatically. However, if we look at docdist6’s profiler output, we notice that merge is the function with the biggest runtime footprint. Merge sort’s performance in practice is great, so it seems that the only way to make our code faster is to get rid of sorting altogether.This iteration switches away from the sorted list representation of document vectors, and in- steadusesthePythondictionaryrepresentationthatwasintroducedindocdist4.count frequency already used that representation internally, so we only need to remove the code that converted the Python dictionary to a list.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return DThis method still takes O(W ) time to process a W -word document.word frequencies for file does not call merge sort anymore, and instead re- turnsthedictionarybuiltbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingNextup,inner productmakesusesdictionarylookupsinsteadofmergingsortedlists.def inner_product(D1,D2): sum = 0.0for key in D1:if key in D2:sum += D1[key] * D2[key]return sum
6.006 Intro to Algorithms Recitation 2 September 11, 2013The logic is quite similar to the straightforward inner product in docdist1. Each word in the first document vector is looked up in the second document vector. However, because the document vectors are dictionaries, each takes O(1) time, and inner product runs in O(L1) time, where L1 is the length of the first document’s vector.docdist7 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 910docdist8At this point, all the algorithms in our solution are asymptotically optimal. We can easily prove this, by noting that each of the 3 main operations runs in time proportional to its input size, and each operation needs to read all its input to produce its output. However, there is still room for optimizing and simplifying the code.There is no reason to read each document line by line, and then break up each line into words. The last iteration processes reads each document into one large string, and breaks up the entire document into words at once.Firstoff,read fileismodifiedtoreturnasinglestring,insteadofanarrayofstrings.Then,get words from line listisrenamedtoget words from file,andissimplified, becauseitdoesn’tneedtoiterateoveralistoflinesanymore.Last,word frequencies for file is updated to reflect the renaming.def get_words_from_string(string):string = string.translate(translation_table) word_list = string.split()return word_listdef word_frequencies_for_file(filename): text = read_file(filename)word_list = get_words_from_string(text) freq_mapping = count_frequency(word_list) return freq_mapping
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist8 Performance ScorecardMethodTimeget words from textcount frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 9101 2 3 4 51 2 3 4 5 6 7 86.006 Intro to Algorithms Recitation 2 September 11, 2013Document Distancedocdist1.py is a straightforward solution to the document distance problem, and docdist{2-8}.py show algorithmic optimizations that improve the running time.We start out by understanding the structure of the straightforward implementation, and then we’ll look at the changes in each of the successive versions.docdist1We first look at main to get a high-level view of what’s going on in the program.def main():if len(sys.argv) != 3:print "Usage: docdist1.py filename_1 filename_2" else:filename_1 = sys.argv[1]filename_2 = sys.argv[2]document_vector_1 = word_frequencies_for_file(filename_1) document_vector_2 = word_frequencies_for_file(filename_2)distance = vector_angle(document_vector_1,document_vector_2)print "The distance between the documents is: %0.6f (radians)"%distanceThemethodprocessesthecommand-linearguments,andcallsword frequencies for file for each document, then calls vector angle on the resulting lists. How do these methods matchupwiththethreeoperationsoutlinedinlecture?Itseemslikeword frequencies for file is responsible for operation 1 (split each document into words) and operation 2 (count word fre- quencies),andthenvector angleisresponsibleforoperation3(computedotproduct).Nextup,we’lltakealookatword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingThe method first calls read file, which returns a list of lines in the input file. We’ll omit the method code, because it is not particularly interesting, and we’ll assume that read file’s running time is proportional to the size of the input file. The input from read line is given to get words from line list, which computes operation 1 (split each document into words).Afterthat,count frequencyturnsthelistofwordsintoadocumentvector(operation 2).def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list = word_list + words_in_line return word_listdef get_words_from_string(line):
 91011121314151617181920212223elif len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word) character_list = []if len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word)return word_list1 2 3 4 5 6 7 8 9101 2 3get words from string takes one line in the input file and breaks it up into a list of words. We iterate through the for loop O(k) times where k is the length of the line. All of the operations within the loop take constant time to run, with the exception of join. Although the join operator takes O(c) time on an input of c characters, each character in the line is joined only once.Therefore,get words from stringhasanoverallruntimeofO(k).get words from line list calls get words from string for each line andcombines the lists into one big list. Line 5 looks innocent but is a big performance killer, becauseusing + to combine W lists of length k is O(W2 ). kkThe output of get words from line list is a list of words, like [’a’, ’cat’,’in’, ’a’, ’bag’].word frequencies from filepassesthisoutputtocount frequency, which turns it into a document vector that counts the number of occurrences of each word, and lookslike[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]].def count_frequency(word_list): L=[]for new_word in word_list: for entry in L:if new_word == entry[0]: entry[1] = entry[1] + 1 breakelse: L.append([new_word,1])return LThe implementation above builds the document vector by takes each word in the input list and looking it up in the list representing the under-construction document vector. In the worst case of a document with all different words, this takes O(W 2 × l) time, where W is the number of words in the document, and l is the average word length.count frequency is the last function call in word frequencies for file. Next up,maincallsvector angle,whichperformsoperation3,computingthedistancemetric.def vector_angle(L1,L2):numerator = inner_product(L1,L2)denominator = math.sqrt(inner_product(L1,L1)*inner_product(L2,L2))6.006 Intro to Algorithmsword_list = [] character_list = [] for c in line:if c.isalnum(): character_list.append(c)Recitation 2September 11, 2013
41 2 3 4 5 6 76.006 Intro to Algorithms Recitation 2 September 11, 2013return math.acos(numerator/denominator)The method is a somewhat straightforward implementation of the distance metric􏰀 L1 · L2 􏰁 􏰄 L1 · L2 􏰅 arccos |L1||L2| = arccos 􏰇(L1 · L1)(L2 · L2),anddelegatestoinner productforthehardworkofcomputingcrossproducts.def inner_product(L1,L2): sum = 0.0for word1, count1 in L1:for word2, count2 in L2: if word1 == word2:sum += count1 * count2 return suminner product is a straightforward inner-product implementation that checks each each word in the first list against the entire second list. The nested loops at lines 3 and 4 give the algorithm its running time of Θ(L1L2), where L1 and L2 are the lengths of the documents’ vectors (the number of unique words in each document).docdist1 Performance ScorecardWe assume that k (number of words per line) is a constant, because the documents will need to fit on screens or paper sheets with a finite length. W (the number of words in a document) is ≥ L (the number of unique words in a document). L21 + L2 + L1L2 = O(L21 + L2) because L21 + L2 ≥ L1L2. Proof (assuming L1, L2 ≥ 0):MethodTimeget words from line list count frequencyword frequencies for fileO(W2 ) = O(W2) kO(WL) O(W2)inner productvector angleO(L1L2) O(L1L2 + L21 + L2) = O(L21 + L2)mainO ( W 12 + W 2 2 )docdist2(L1−L2)2 ≥0 L21+L2−2L1L2 ≥ 0L21+L2 ≥ 2L1L2 L 21 + L 2 2 ≥ L 1 L 2The document distance code invokes the Python profiler to identify the code that takes up the most CPU time. This ensures that we get the biggest returns on our optimization efforts.
1 2 36.006 Intro to Algorithms Recitation 2 September 11, 2013if __name__ == "__main__": import cProfile cProfile.run("main()")You can profile existing programs without changing them by adding -m cProfile -s Time to Python’s command line. For example, the command below will run and profile program.py.  python -m cProfile -s time program.pyA sample profiler output is shown below.123456 ncalls tottime 7 1 0.001 8 2 0.000 9 3 6.25510 1 0.000 11 1 0.004 12 2 0.000 13 2 16.995 14 22663 0.793 15 2 11.205 16 325087 0.034 17 1 0.000 18 1 0.000 19 1241849 0.145 20 1 0.000 21 1300248 0.112 22 232140 0.054 23 232140 0.035 24 2 0.003 25 2 0.000The distance between the documents is: 0.574160 (radians) 3354148 function calls in 35.636 CPU secondsOrdered by: standard namepercall  cumtime  0.001   35.636  0.000   29.376  2.085    6.255  0.000    6.255  0.004   35.635  0.000    0.003  8.498   18.167  0.000    1.172  5.602   11.206  0.000    0.034  0.000    0.000  0.000    0.000  0.000    0.145  0.000    0.000  0.000    0.112  0.000    0.0540.000 0.0350.001 0.0030.000 0.000percall filename:lineno(function) 35.636 <string>:1(<module>) 14.688 docdist1.py:100(word_frequencies_for_file)  2.085 docdist1.py:110(inner_product)  6.255 docdist1.py:125(vector_angle) 35.635 docdist1.py:135(main)  0.001 docdist1.py:37(read_file)  9.083 docdist1.py:49(get_words_from_line_list)  0.000 docdist1.py:61(get_words_from_string)  5.603 docdist1.py:86(count_frequency)  0.000 {len}  0.000 {math.acos}  0.000 {math.sqrt}  0.000 {method ’append’ of ’list’ objects}  0.000 {method ’disable’ of ’_lsprof.Profiler’ objec  0.000 {method ’isalnum’ of ’str’ objects}  0.000 {method ’join’ of ’str’ objects}  0.000 {method ’lower’ of ’str’ objects}  0.001 {method ’readlines’ of ’file’ objects}  0.000 {open}1 2 3 4 5 6Theprofileroutputfordocdist1showsthatthebiggesttimedrainisget words from line list. The problem is that when the + operator is used to concatenate lists, it needs to create a new listand copy the elements of both its operands. Replacing + with extend yields a 30% runtime improvement.def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list.extend(words_in_line) return word_listextend adds all the elements of an m-element list to an n-element list in Θ(1+m), as opposed to +, which needs to create a new list, and therefore takes Θ(1 + n + m) time. So concatenatingkW lists of k elements takes 􏰆W k = Θ(W) time.k
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist2 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(WL) O(WL)inner productvector angleO(L1L2) O ( L 21 + L 2 2 )mainO(W1L1 + W2L2)1 2 3 4 5 61 2 3 4 5 6 7 8 91 2 3 4 5 6 7docdist3Profilingdocdist2pointstocount frequencyandinner productasgoodtargetsforop- timizations. We’ll speed up inner product by switching to a fast algorithm. However, the algorithmassumesthatthewordsinthedocumentvectorsaresorted.Forexample,[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]]needstobecome[[’a’, 2], [’bag’, 1], [’cat’, 1], [’in’, 1]].Firstoff,weaddasteptoword frequencies for filethatsortsthedocumentvector producedbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) insertion_sort(freq_mapping)return freq_mappingThenweimplementinsertion sortusingthealgorithminthetextbook.def insertion_sort(A): for j in range(len(A)):key = A[j]i = j-1while i>-1 and A[i]>key:A[i+1] = A[i]      i = i-1    A[i+1] = keyreturn AInsertion sort runs in O(L2) time, where L is the length of the document vector to be sorted.Finally,inner productisre-implementedusingasimilaralgorithmtothemergingstepin Merge Sort.def inner_product(L1,L2): sum = 0.0i=0j=0while i<len(L1) and j<len(L2):# L1[i:] and L2[j:] yet to be processed if L1[i][0] == L2[j][0]:
8 91011121314151617186.006 Intro to Algorithms Recitation 2 September 11, 2013      # both vectors have this word      sum += L1[i][1] * L2[j][1]      i += 1      j += 1elif L1[i][0] < L2[j][0]:# word L1[i][0] is in L1 but not L2 i += 1else:# word L2[j][0] is in L2 but not L1 j += 1return sumThe new implementation runs in Θ(L1 + L2), where L1 and L2 are the lengths of the two document vectors. We observe that the running time for inner product (and therefore for vector angle)isasymptoticallyoptimal,becauseanyalgorithmthatcomputestheinnerprod- uct will have to read the two vectors, and reading will take Ω(L1 + L2) time.docdist3 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(WL)O(L2) O(WL + L2) = O(WL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1L1 + W2L2)1 2 3 4 5 6 7 8docdist4The next iteration addresses count frequency, which is the biggest time consumer at the moment.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return D.items()The new implementation uses Python dictionaries. The dictionaries are implemented using hash tables, which will be presented in a future lecture. The salient feature of hash tables is that inserting an element using dictionary[key] = value and looking up an element using dictionary[key] both run in O(1) expected time.Instead of storing the document vector under construction in a list, the new implementation uses a dictionary. The keys are the words in the document, and the value are the number of times
6.006 Intro to Algorithms Recitation 2 September 11, 2013 each word appears in the document. Since both insertion (line 5) and lookup (line 7) take O(1)time, building a document vector out of W words takes O(W ) time. docdist4 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(W)O(L2) O(W + L2) = O(L2)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L21 +L2)1 2 3 4 5 6 7docdist5Thisiterationsimplifiesget words from stringthatbreaksuplinesintowords.Firstoff, the standard library function string.translate is used for converting uppercase characters to lowercase, and for converting punctuation to spaces. Second, the split method on strings is used to break up a line into words.translation_table = string.maketrans(string.punctuation+string.uppercase,                   " "*len(string.punctuation)+string.lowercase)def get_words_from_string(line):line = line.translate(translation_table) word_list = line.split()return word_listThe main benefit of this change is that 23 lines of code are replaced with 5 lines of code. This makes the implementation easier to analyze. A side benefit is that many functions in the Python standard library are implemented in directly in C (a low-level programming language that is very close to machine code), which gives them better performance. Although the running time is asymptotically the same, the hidden constants are much better for the C code than for our Python code presented in docdist1.docdist5 Performance ScorecardIdentical to the docdist4 scorecard.docdist6Nowthatallthedistractionsareoutoftheway,it’stimetotackleinsertion sort,whichis takes up the most CPU time, by far, in the profiler output for docdist5.The advantages of insertion sort are that it sorts in place, and it is simple to implement. However, its worst-case running time is O(N2) for an N-element array. We’ll replace insertion
1 2 3 4 5 61 2 3 4 5 6 7 8 9101112131415161718192021222324256.006 Intro to Algorithms Recitation 2 September 11, 2013 sort with a better algorithm, merge sort. Merge sort is not in place, so we’ll need to modifyword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) freq_mapping = merge_sort(freq_mapping)return freq_mappingThemerge sortimplementationcloselyfollowsthepseudocodeinthetextbook.def merge_sort(A): n = len(A)if n==1:return Amid = n//2L = merge_sort(A[:mid]) R = merge_sort(A[mid:]) return merge(L,R)def merge(L,R): i=0j=0answer = []while i<len(L) and j<len(R):if L[i]<R[j]: answer.append(L[i]) i += 1else: answer.append(R[j]) j += 1if i<len(L): answer.extend(L[i:])if j<len(R): answer.extend(R[j:])return answerThe textbook proves that merge sort runs in Θ(n log n) time. You should apply your knowledge of the Python cost model to convince yourself that the implementation above also runs in Θ(n log n) time.
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist6 Performance ScorecardMethodTimeget words from line list count frequencymerge sortword frequencies for fileO(W)O(W) O(L log L) O(W +LlogL)=O(LlogL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L1 logL1 +L2 logL2)1 2 3 4 5 6 7 81 2 3 4 51 2 3 4 5 6docdist7Switching to merge sort improved the running time dramatically. However, if we look at docdist6’s profiler output, we notice that merge is the function with the biggest runtime footprint. Merge sort’s performance in practice is great, so it seems that the only way to make our code faster is to get rid of sorting altogether.This iteration switches away from the sorted list representation of document vectors, and in- steadusesthePythondictionaryrepresentationthatwasintroducedindocdist4.count frequency already used that representation internally, so we only need to remove the code that converted the Python dictionary to a list.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return DThis method still takes O(W ) time to process a W -word document.word frequencies for file does not call merge sort anymore, and instead re- turnsthedictionarybuiltbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingNextup,inner productmakesusesdictionarylookupsinsteadofmergingsortedlists.def inner_product(D1,D2): sum = 0.0for key in D1:if key in D2:sum += D1[key] * D2[key]return sum
6.006 Intro to Algorithms Recitation 2 September 11, 2013The logic is quite similar to the straightforward inner product in docdist1. Each word in the first document vector is looked up in the second document vector. However, because the document vectors are dictionaries, each takes O(1) time, and inner product runs in O(L1) time, where L1 is the length of the first document’s vector.docdist7 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 910docdist8At this point, all the algorithms in our solution are asymptotically optimal. We can easily prove this, by noting that each of the 3 main operations runs in time proportional to its input size, and each operation needs to read all its input to produce its output. However, there is still room for optimizing and simplifying the code.There is no reason to read each document line by line, and then break up each line into words. The last iteration processes reads each document into one large string, and breaks up the entire document into words at once.Firstoff,read fileismodifiedtoreturnasinglestring,insteadofanarrayofstrings.Then,get words from line listisrenamedtoget words from file,andissimplified, becauseitdoesn’tneedtoiterateoveralistoflinesanymore.Last,word frequencies for file is updated to reflect the renaming.def get_words_from_string(string):string = string.translate(translation_table) word_list = string.split()return word_listdef word_frequencies_for_file(filename): text = read_file(filename)word_list = get_words_from_string(text) freq_mapping = count_frequency(word_list) return freq_mapping
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist8 Performance ScorecardMethodTimeget words from textcount frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 9101 2 3 4 51 2 3 4 5 6 7 86.006 Intro to Algorithms Recitation 2 September 11, 2013Document Distancedocdist1.py is a straightforward solution to the document distance problem, and docdist{2-8}.py show algorithmic optimizations that improve the running time.We start out by understanding the structure of the straightforward implementation, and then we’ll look at the changes in each of the successive versions.docdist1We first look at main to get a high-level view of what’s going on in the program.def main():if len(sys.argv) != 3:print "Usage: docdist1.py filename_1 filename_2" else:filename_1 = sys.argv[1]filename_2 = sys.argv[2]document_vector_1 = word_frequencies_for_file(filename_1) document_vector_2 = word_frequencies_for_file(filename_2)distance = vector_angle(document_vector_1,document_vector_2)print "The distance between the documents is: %0.6f (radians)"%distanceThemethodprocessesthecommand-linearguments,andcallsword frequencies for file for each document, then calls vector angle on the resulting lists. How do these methods matchupwiththethreeoperationsoutlinedinlecture?Itseemslikeword frequencies for file is responsible for operation 1 (split each document into words) and operation 2 (count word fre- quencies),andthenvector angleisresponsibleforoperation3(computedotproduct).Nextup,we’lltakealookatword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingThe method first calls read file, which returns a list of lines in the input file. We’ll omit the method code, because it is not particularly interesting, and we’ll assume that read file’s running time is proportional to the size of the input file. The input from read line is given to get words from line list, which computes operation 1 (split each document into words).Afterthat,count frequencyturnsthelistofwordsintoadocumentvector(operation 2).def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list = word_list + words_in_line return word_listdef get_words_from_string(line):
 91011121314151617181920212223elif len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word) character_list = []if len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word)return word_list1 2 3 4 5 6 7 8 9101 2 3get words from string takes one line in the input file and breaks it up into a list of words. We iterate through the for loop O(k) times where k is the length of the line. All of the operations within the loop take constant time to run, with the exception of join. Although the join operator takes O(c) time on an input of c characters, each character in the line is joined only once.Therefore,get words from stringhasanoverallruntimeofO(k).get words from line list calls get words from string for each line andcombines the lists into one big list. Line 5 looks innocent but is a big performance killer, becauseusing + to combine W lists of length k is O(W2 ). kkThe output of get words from line list is a list of words, like [’a’, ’cat’,’in’, ’a’, ’bag’].word frequencies from filepassesthisoutputtocount frequency, which turns it into a document vector that counts the number of occurrences of each word, and lookslike[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]].def count_frequency(word_list): L=[]for new_word in word_list: for entry in L:if new_word == entry[0]: entry[1] = entry[1] + 1 breakelse: L.append([new_word,1])return LThe implementation above builds the document vector by takes each word in the input list and looking it up in the list representing the under-construction document vector. In the worst case of a document with all different words, this takes O(W 2 × l) time, where W is the number of words in the document, and l is the average word length.count frequency is the last function call in word frequencies for file. Next up,maincallsvector angle,whichperformsoperation3,computingthedistancemetric.def vector_angle(L1,L2):numerator = inner_product(L1,L2)denominator = math.sqrt(inner_product(L1,L1)*inner_product(L2,L2))6.006 Intro to Algorithmsword_list = [] character_list = [] for c in line:if c.isalnum(): character_list.append(c)Recitation 2September 11, 2013
41 2 3 4 5 6 76.006 Intro to Algorithms Recitation 2 September 11, 2013return math.acos(numerator/denominator)The method is a somewhat straightforward implementation of the distance metric􏰀 L1 · L2 􏰁 􏰄 L1 · L2 􏰅 arccos |L1||L2| = arccos 􏰇(L1 · L1)(L2 · L2),anddelegatestoinner productforthehardworkofcomputingcrossproducts.def inner_product(L1,L2): sum = 0.0for word1, count1 in L1:for word2, count2 in L2: if word1 == word2:sum += count1 * count2 return suminner product is a straightforward inner-product implementation that checks each each word in the first list against the entire second list. The nested loops at lines 3 and 4 give the algorithm its running time of Θ(L1L2), where L1 and L2 are the lengths of the documents’ vectors (the number of unique words in each document).docdist1 Performance ScorecardWe assume that k (number of words per line) is a constant, because the documents will need to fit on screens or paper sheets with a finite length. W (the number of words in a document) is ≥ L (the number of unique words in a document). L21 + L2 + L1L2 = O(L21 + L2) because L21 + L2 ≥ L1L2. Proof (assuming L1, L2 ≥ 0):MethodTimeget words from line list count frequencyword frequencies for fileO(W2 ) = O(W2) kO(WL) O(W2)inner productvector angleO(L1L2) O(L1L2 + L21 + L2) = O(L21 + L2)mainO ( W 12 + W 2 2 )docdist2(L1−L2)2 ≥0 L21+L2−2L1L2 ≥ 0L21+L2 ≥ 2L1L2 L 21 + L 2 2 ≥ L 1 L 2The document distance code invokes the Python profiler to identify the code that takes up the most CPU time. This ensures that we get the biggest returns on our optimization efforts.
1 2 36.006 Intro to Algorithms Recitation 2 September 11, 2013if __name__ == "__main__": import cProfile cProfile.run("main()")You can profile existing programs without changing them by adding -m cProfile -s Time to Python’s command line. For example, the command below will run and profile program.py.  python -m cProfile -s time program.pyA sample profiler output is shown below.123456 ncalls tottime 7 1 0.001 8 2 0.000 9 3 6.25510 1 0.000 11 1 0.004 12 2 0.000 13 2 16.995 14 22663 0.793 15 2 11.205 16 325087 0.034 17 1 0.000 18 1 0.000 19 1241849 0.145 20 1 0.000 21 1300248 0.112 22 232140 0.054 23 232140 0.035 24 2 0.003 25 2 0.000The distance between the documents is: 0.574160 (radians) 3354148 function calls in 35.636 CPU secondsOrdered by: standard namepercall  cumtime  0.001   35.636  0.000   29.376  2.085    6.255  0.000    6.255  0.004   35.635  0.000    0.003  8.498   18.167  0.000    1.172  5.602   11.206  0.000    0.034  0.000    0.000  0.000    0.000  0.000    0.145  0.000    0.000  0.000    0.112  0.000    0.0540.000 0.0350.001 0.0030.000 0.000percall filename:lineno(function) 35.636 <string>:1(<module>) 14.688 docdist1.py:100(word_frequencies_for_file)  2.085 docdist1.py:110(inner_product)  6.255 docdist1.py:125(vector_angle) 35.635 docdist1.py:135(main)  0.001 docdist1.py:37(read_file)  9.083 docdist1.py:49(get_words_from_line_list)  0.000 docdist1.py:61(get_words_from_string)  5.603 docdist1.py:86(count_frequency)  0.000 {len}  0.000 {math.acos}  0.000 {math.sqrt}  0.000 {method ’append’ of ’list’ objects}  0.000 {method ’disable’ of ’_lsprof.Profiler’ objec  0.000 {method ’isalnum’ of ’str’ objects}  0.000 {method ’join’ of ’str’ objects}  0.000 {method ’lower’ of ’str’ objects}  0.001 {method ’readlines’ of ’file’ objects}  0.000 {open}1 2 3 4 5 6Theprofileroutputfordocdist1showsthatthebiggesttimedrainisget words from line list. The problem is that when the + operator is used to concatenate lists, it needs to create a new listand copy the elements of both its operands. Replacing + with extend yields a 30% runtime improvement.def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list.extend(words_in_line) return word_listextend adds all the elements of an m-element list to an n-element list in Θ(1+m), as opposed to +, which needs to create a new list, and therefore takes Θ(1 + n + m) time. So concatenatingkW lists of k elements takes 􏰆W k = Θ(W) time.k
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist2 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(WL) O(WL)inner productvector angleO(L1L2) O ( L 21 + L 2 2 )mainO(W1L1 + W2L2)1 2 3 4 5 61 2 3 4 5 6 7 8 91 2 3 4 5 6 7docdist3Profilingdocdist2pointstocount frequencyandinner productasgoodtargetsforop- timizations. We’ll speed up inner product by switching to a fast algorithm. However, the algorithmassumesthatthewordsinthedocumentvectorsaresorted.Forexample,[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]]needstobecome[[’a’, 2], [’bag’, 1], [’cat’, 1], [’in’, 1]].Firstoff,weaddasteptoword frequencies for filethatsortsthedocumentvector producedbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) insertion_sort(freq_mapping)return freq_mappingThenweimplementinsertion sortusingthealgorithminthetextbook.def insertion_sort(A): for j in range(len(A)):key = A[j]i = j-1while i>-1 and A[i]>key:A[i+1] = A[i]      i = i-1    A[i+1] = keyreturn AInsertion sort runs in O(L2) time, where L is the length of the document vector to be sorted.Finally,inner productisre-implementedusingasimilaralgorithmtothemergingstepin Merge Sort.def inner_product(L1,L2): sum = 0.0i=0j=0while i<len(L1) and j<len(L2):# L1[i:] and L2[j:] yet to be processed if L1[i][0] == L2[j][0]:
8 91011121314151617186.006 Intro to Algorithms Recitation 2 September 11, 2013      # both vectors have this word      sum += L1[i][1] * L2[j][1]      i += 1      j += 1elif L1[i][0] < L2[j][0]:# word L1[i][0] is in L1 but not L2 i += 1else:# word L2[j][0] is in L2 but not L1 j += 1return sumThe new implementation runs in Θ(L1 + L2), where L1 and L2 are the lengths of the two document vectors. We observe that the running time for inner product (and therefore for vector angle)isasymptoticallyoptimal,becauseanyalgorithmthatcomputestheinnerprod- uct will have to read the two vectors, and reading will take Ω(L1 + L2) time.docdist3 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(WL)O(L2) O(WL + L2) = O(WL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1L1 + W2L2)1 2 3 4 5 6 7 8docdist4The next iteration addresses count frequency, which is the biggest time consumer at the moment.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return D.items()The new implementation uses Python dictionaries. The dictionaries are implemented using hash tables, which will be presented in a future lecture. The salient feature of hash tables is that inserting an element using dictionary[key] = value and looking up an element using dictionary[key] both run in O(1) expected time.Instead of storing the document vector under construction in a list, the new implementation uses a dictionary. The keys are the words in the document, and the value are the number of times
6.006 Intro to Algorithms Recitation 2 September 11, 2013 each word appears in the document. Since both insertion (line 5) and lookup (line 7) take O(1)time, building a document vector out of W words takes O(W ) time. docdist4 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(W)O(L2) O(W + L2) = O(L2)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L21 +L2)1 2 3 4 5 6 7docdist5Thisiterationsimplifiesget words from stringthatbreaksuplinesintowords.Firstoff, the standard library function string.translate is used for converting uppercase characters to lowercase, and for converting punctuation to spaces. Second, the split method on strings is used to break up a line into words.translation_table = string.maketrans(string.punctuation+string.uppercase,                   " "*len(string.punctuation)+string.lowercase)def get_words_from_string(line):line = line.translate(translation_table) word_list = line.split()return word_listThe main benefit of this change is that 23 lines of code are replaced with 5 lines of code. This makes the implementation easier to analyze. A side benefit is that many functions in the Python standard library are implemented in directly in C (a low-level programming language that is very close to machine code), which gives them better performance. Although the running time is asymptotically the same, the hidden constants are much better for the C code than for our Python code presented in docdist1.docdist5 Performance ScorecardIdentical to the docdist4 scorecard.docdist6Nowthatallthedistractionsareoutoftheway,it’stimetotackleinsertion sort,whichis takes up the most CPU time, by far, in the profiler output for docdist5.The advantages of insertion sort are that it sorts in place, and it is simple to implement. However, its worst-case running time is O(N2) for an N-element array. We’ll replace insertion
1 2 3 4 5 61 2 3 4 5 6 7 8 9101112131415161718192021222324256.006 Intro to Algorithms Recitation 2 September 11, 2013 sort with a better algorithm, merge sort. Merge sort is not in place, so we’ll need to modifyword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) freq_mapping = merge_sort(freq_mapping)return freq_mappingThemerge sortimplementationcloselyfollowsthepseudocodeinthetextbook.def merge_sort(A): n = len(A)if n==1:return Amid = n//2L = merge_sort(A[:mid]) R = merge_sort(A[mid:]) return merge(L,R)def merge(L,R): i=0j=0answer = []while i<len(L) and j<len(R):if L[i]<R[j]: answer.append(L[i]) i += 1else: answer.append(R[j]) j += 1if i<len(L): answer.extend(L[i:])if j<len(R): answer.extend(R[j:])return answerThe textbook proves that merge sort runs in Θ(n log n) time. You should apply your knowledge of the Python cost model to convince yourself that the implementation above also runs in Θ(n log n) time.
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist6 Performance ScorecardMethodTimeget words from line list count frequencymerge sortword frequencies for fileO(W)O(W) O(L log L) O(W +LlogL)=O(LlogL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L1 logL1 +L2 logL2)1 2 3 4 5 6 7 81 2 3 4 51 2 3 4 5 6docdist7Switching to merge sort improved the running time dramatically. However, if we look at docdist6’s profiler output, we notice that merge is the function with the biggest runtime footprint. Merge sort’s performance in practice is great, so it seems that the only way to make our code faster is to get rid of sorting altogether.This iteration switches away from the sorted list representation of document vectors, and in- steadusesthePythondictionaryrepresentationthatwasintroducedindocdist4.count frequency already used that representation internally, so we only need to remove the code that converted the Python dictionary to a list.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return DThis method still takes O(W ) time to process a W -word document.word frequencies for file does not call merge sort anymore, and instead re- turnsthedictionarybuiltbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingNextup,inner productmakesusesdictionarylookupsinsteadofmergingsortedlists.def inner_product(D1,D2): sum = 0.0for key in D1:if key in D2:sum += D1[key] * D2[key]return sum
6.006 Intro to Algorithms Recitation 2 September 11, 2013The logic is quite similar to the straightforward inner product in docdist1. Each word in the first document vector is looked up in the second document vector. However, because the document vectors are dictionaries, each takes O(1) time, and inner product runs in O(L1) time, where L1 is the length of the first document’s vector.docdist7 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 910docdist8At this point, all the algorithms in our solution are asymptotically optimal. We can easily prove this, by noting that each of the 3 main operations runs in time proportional to its input size, and each operation needs to read all its input to produce its output. However, there is still room for optimizing and simplifying the code.There is no reason to read each document line by line, and then break up each line into words. The last iteration processes reads each document into one large string, and breaks up the entire document into words at once.Firstoff,read fileismodifiedtoreturnasinglestring,insteadofanarrayofstrings.Then,get words from line listisrenamedtoget words from file,andissimplified, becauseitdoesn’tneedtoiterateoveralistoflinesanymore.Last,word frequencies for file is updated to reflect the renaming.def get_words_from_string(string):string = string.translate(translation_table) word_list = string.split()return word_listdef word_frequencies_for_file(filename): text = read_file(filename)word_list = get_words_from_string(text) freq_mapping = count_frequency(word_list) return freq_mapping
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist8 Performance ScorecardMethodTimeget words from textcount frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 9101 2 3 4 51 2 3 4 5 6 7 86.006 Intro to Algorithms Recitation 2 September 11, 2013Document Distancedocdist1.py is a straightforward solution to the document distance problem, and docdist{2-8}.py show algorithmic optimizations that improve the running time.We start out by understanding the structure of the straightforward implementation, and then we’ll look at the changes in each of the successive versions.docdist1We first look at main to get a high-level view of what’s going on in the program.def main():if len(sys.argv) != 3:print "Usage: docdist1.py filename_1 filename_2" else:filename_1 = sys.argv[1]filename_2 = sys.argv[2]document_vector_1 = word_frequencies_for_file(filename_1) document_vector_2 = word_frequencies_for_file(filename_2)distance = vector_angle(document_vector_1,document_vector_2)print "The distance between the documents is: %0.6f (radians)"%distanceThemethodprocessesthecommand-linearguments,andcallsword frequencies for file for each document, then calls vector angle on the resulting lists. How do these methods matchupwiththethreeoperationsoutlinedinlecture?Itseemslikeword frequencies for file is responsible for operation 1 (split each document into words) and operation 2 (count word fre- quencies),andthenvector angleisresponsibleforoperation3(computedotproduct).Nextup,we’lltakealookatword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingThe method first calls read file, which returns a list of lines in the input file. We’ll omit the method code, because it is not particularly interesting, and we’ll assume that read file’s running time is proportional to the size of the input file. The input from read line is given to get words from line list, which computes operation 1 (split each document into words).Afterthat,count frequencyturnsthelistofwordsintoadocumentvector(operation 2).def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list = word_list + words_in_line return word_listdef get_words_from_string(line):
 91011121314151617181920212223elif len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word) character_list = []if len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word)return word_list1 2 3 4 5 6 7 8 9101 2 3get words from string takes one line in the input file and breaks it up into a list of words. We iterate through the for loop O(k) times where k is the length of the line. All of the operations within the loop take constant time to run, with the exception of join. Although the join operator takes O(c) time on an input of c characters, each character in the line is joined only once.Therefore,get words from stringhasanoverallruntimeofO(k).get words from line list calls get words from string for each line andcombines the lists into one big list. Line 5 looks innocent but is a big performance killer, becauseusing + to combine W lists of length k is O(W2 ). kkThe output of get words from line list is a list of words, like [’a’, ’cat’,’in’, ’a’, ’bag’].word frequencies from filepassesthisoutputtocount frequency, which turns it into a document vector that counts the number of occurrences of each word, and lookslike[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]].def count_frequency(word_list): L=[]for new_word in word_list: for entry in L:if new_word == entry[0]: entry[1] = entry[1] + 1 breakelse: L.append([new_word,1])return LThe implementation above builds the document vector by takes each word in the input list and looking it up in the list representing the under-construction document vector. In the worst case of a document with all different words, this takes O(W 2 × l) time, where W is the number of words in the document, and l is the average word length.count frequency is the last function call in word frequencies for file. Next up,maincallsvector angle,whichperformsoperation3,computingthedistancemetric.def vector_angle(L1,L2):numerator = inner_product(L1,L2)denominator = math.sqrt(inner_product(L1,L1)*inner_product(L2,L2))6.006 Intro to Algorithmsword_list = [] character_list = [] for c in line:if c.isalnum(): character_list.append(c)Recitation 2September 11, 2013
41 2 3 4 5 6 76.006 Intro to Algorithms Recitation 2 September 11, 2013return math.acos(numerator/denominator)The method is a somewhat straightforward implementation of the distance metric􏰀 L1 · L2 􏰁 􏰄 L1 · L2 􏰅 arccos |L1||L2| = arccos 􏰇(L1 · L1)(L2 · L2),anddelegatestoinner productforthehardworkofcomputingcrossproducts.def inner_product(L1,L2): sum = 0.0for word1, count1 in L1:for word2, count2 in L2: if word1 == word2:sum += count1 * count2 return suminner product is a straightforward inner-product implementation that checks each each word in the first list against the entire second list. The nested loops at lines 3 and 4 give the algorithm its running time of Θ(L1L2), where L1 and L2 are the lengths of the documents’ vectors (the number of unique words in each document).docdist1 Performance ScorecardWe assume that k (number of words per line) is a constant, because the documents will need to fit on screens or paper sheets with a finite length. W (the number of words in a document) is ≥ L (the number of unique words in a document). L21 + L2 + L1L2 = O(L21 + L2) because L21 + L2 ≥ L1L2. Proof (assuming L1, L2 ≥ 0):MethodTimeget words from line list count frequencyword frequencies for fileO(W2 ) = O(W2) kO(WL) O(W2)inner productvector angleO(L1L2) O(L1L2 + L21 + L2) = O(L21 + L2)mainO ( W 12 + W 2 2 )docdist2(L1−L2)2 ≥0 L21+L2−2L1L2 ≥ 0L21+L2 ≥ 2L1L2 L 21 + L 2 2 ≥ L 1 L 2The document distance code invokes the Python profiler to identify the code that takes up the most CPU time. This ensures that we get the biggest returns on our optimization efforts.
1 2 36.006 Intro to Algorithms Recitation 2 September 11, 2013if __name__ == "__main__": import cProfile cProfile.run("main()")You can profile existing programs without changing them by adding -m cProfile -s Time to Python’s command line. For example, the command below will run and profile program.py.  python -m cProfile -s time program.pyA sample profiler output is shown below.123456 ncalls tottime 7 1 0.001 8 2 0.000 9 3 6.25510 1 0.000 11 1 0.004 12 2 0.000 13 2 16.995 14 22663 0.793 15 2 11.205 16 325087 0.034 17 1 0.000 18 1 0.000 19 1241849 0.145 20 1 0.000 21 1300248 0.112 22 232140 0.054 23 232140 0.035 24 2 0.003 25 2 0.000The distance between the documents is: 0.574160 (radians) 3354148 function calls in 35.636 CPU secondsOrdered by: standard namepercall  cumtime  0.001   35.636  0.000   29.376  2.085    6.255  0.000    6.255  0.004   35.635  0.000    0.003  8.498   18.167  0.000    1.172  5.602   11.206  0.000    0.034  0.000    0.000  0.000    0.000  0.000    0.145  0.000    0.000  0.000    0.112  0.000    0.0540.000 0.0350.001 0.0030.000 0.000percall filename:lineno(function) 35.636 <string>:1(<module>) 14.688 docdist1.py:100(word_frequencies_for_file)  2.085 docdist1.py:110(inner_product)  6.255 docdist1.py:125(vector_angle) 35.635 docdist1.py:135(main)  0.001 docdist1.py:37(read_file)  9.083 docdist1.py:49(get_words_from_line_list)  0.000 docdist1.py:61(get_words_from_string)  5.603 docdist1.py:86(count_frequency)  0.000 {len}  0.000 {math.acos}  0.000 {math.sqrt}  0.000 {method ’append’ of ’list’ objects}  0.000 {method ’disable’ of ’_lsprof.Profiler’ objec  0.000 {method ’isalnum’ of ’str’ objects}  0.000 {method ’join’ of ’str’ objects}  0.000 {method ’lower’ of ’str’ objects}  0.001 {method ’readlines’ of ’file’ objects}  0.000 {open}1 2 3 4 5 6Theprofileroutputfordocdist1showsthatthebiggesttimedrainisget words from line list. The problem is that when the + operator is used to concatenate lists, it needs to create a new listand copy the elements of both its operands. Replacing + with extend yields a 30% runtime improvement.def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list.extend(words_in_line) return word_listextend adds all the elements of an m-element list to an n-element list in Θ(1+m), as opposed to +, which needs to create a new list, and therefore takes Θ(1 + n + m) time. So concatenatingkW lists of k elements takes 􏰆W k = Θ(W) time.k
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist2 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(WL) O(WL)inner productvector angleO(L1L2) O ( L 21 + L 2 2 )mainO(W1L1 + W2L2)1 2 3 4 5 61 2 3 4 5 6 7 8 91 2 3 4 5 6 7docdist3Profilingdocdist2pointstocount frequencyandinner productasgoodtargetsforop- timizations. We’ll speed up inner product by switching to a fast algorithm. However, the algorithmassumesthatthewordsinthedocumentvectorsaresorted.Forexample,[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]]needstobecome[[’a’, 2], [’bag’, 1], [’cat’, 1], [’in’, 1]].Firstoff,weaddasteptoword frequencies for filethatsortsthedocumentvector producedbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) insertion_sort(freq_mapping)return freq_mappingThenweimplementinsertion sortusingthealgorithminthetextbook.def insertion_sort(A): for j in range(len(A)):key = A[j]i = j-1while i>-1 and A[i]>key:A[i+1] = A[i]      i = i-1    A[i+1] = keyreturn AInsertion sort runs in O(L2) time, where L is the length of the document vector to be sorted.Finally,inner productisre-implementedusingasimilaralgorithmtothemergingstepin Merge Sort.def inner_product(L1,L2): sum = 0.0i=0j=0while i<len(L1) and j<len(L2):# L1[i:] and L2[j:] yet to be processed if L1[i][0] == L2[j][0]:
8 91011121314151617186.006 Intro to Algorithms Recitation 2 September 11, 2013      # both vectors have this word      sum += L1[i][1] * L2[j][1]      i += 1      j += 1elif L1[i][0] < L2[j][0]:# word L1[i][0] is in L1 but not L2 i += 1else:# word L2[j][0] is in L2 but not L1 j += 1return sumThe new implementation runs in Θ(L1 + L2), where L1 and L2 are the lengths of the two document vectors. We observe that the running time for inner product (and therefore for vector angle)isasymptoticallyoptimal,becauseanyalgorithmthatcomputestheinnerprod- uct will have to read the two vectors, and reading will take Ω(L1 + L2) time.docdist3 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(WL)O(L2) O(WL + L2) = O(WL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1L1 + W2L2)1 2 3 4 5 6 7 8docdist4The next iteration addresses count frequency, which is the biggest time consumer at the moment.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return D.items()The new implementation uses Python dictionaries. The dictionaries are implemented using hash tables, which will be presented in a future lecture. The salient feature of hash tables is that inserting an element using dictionary[key] = value and looking up an element using dictionary[key] both run in O(1) expected time.Instead of storing the document vector under construction in a list, the new implementation uses a dictionary. The keys are the words in the document, and the value are the number of times
6.006 Intro to Algorithms Recitation 2 September 11, 2013 each word appears in the document. Since both insertion (line 5) and lookup (line 7) take O(1)time, building a document vector out of W words takes O(W ) time. docdist4 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(W)O(L2) O(W + L2) = O(L2)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L21 +L2)1 2 3 4 5 6 7docdist5Thisiterationsimplifiesget words from stringthatbreaksuplinesintowords.Firstoff, the standard library function string.translate is used for converting uppercase characters to lowercase, and for converting punctuation to spaces. Second, the split method on strings is used to break up a line into words.translation_table = string.maketrans(string.punctuation+string.uppercase,                   " "*len(string.punctuation)+string.lowercase)def get_words_from_string(line):line = line.translate(translation_table) word_list = line.split()return word_listThe main benefit of this change is that 23 lines of code are replaced with 5 lines of code. This makes the implementation easier to analyze. A side benefit is that many functions in the Python standard library are implemented in directly in C (a low-level programming language that is very close to machine code), which gives them better performance. Although the running time is asymptotically the same, the hidden constants are much better for the C code than for our Python code presented in docdist1.docdist5 Performance ScorecardIdentical to the docdist4 scorecard.docdist6Nowthatallthedistractionsareoutoftheway,it’stimetotackleinsertion sort,whichis takes up the most CPU time, by far, in the profiler output for docdist5.The advantages of insertion sort are that it sorts in place, and it is simple to implement. However, its worst-case running time is O(N2) for an N-element array. We’ll replace insertion
1 2 3 4 5 61 2 3 4 5 6 7 8 9101112131415161718192021222324256.006 Intro to Algorithms Recitation 2 September 11, 2013 sort with a better algorithm, merge sort. Merge sort is not in place, so we’ll need to modifyword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) freq_mapping = merge_sort(freq_mapping)return freq_mappingThemerge sortimplementationcloselyfollowsthepseudocodeinthetextbook.def merge_sort(A): n = len(A)if n==1:return Amid = n//2L = merge_sort(A[:mid]) R = merge_sort(A[mid:]) return merge(L,R)def merge(L,R): i=0j=0answer = []while i<len(L) and j<len(R):if L[i]<R[j]: answer.append(L[i]) i += 1else: answer.append(R[j]) j += 1if i<len(L): answer.extend(L[i:])if j<len(R): answer.extend(R[j:])return answerThe textbook proves that merge sort runs in Θ(n log n) time. You should apply your knowledge of the Python cost model to convince yourself that the implementation above also runs in Θ(n log n) time.
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist6 Performance ScorecardMethodTimeget words from line list count frequencymerge sortword frequencies for fileO(W)O(W) O(L log L) O(W +LlogL)=O(LlogL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L1 logL1 +L2 logL2)1 2 3 4 5 6 7 81 2 3 4 51 2 3 4 5 6docdist7Switching to merge sort improved the running time dramatically. However, if we look at docdist6’s profiler output, we notice that merge is the function with the biggest runtime footprint. Merge sort’s performance in practice is great, so it seems that the only way to make our code faster is to get rid of sorting altogether.This iteration switches away from the sorted list representation of document vectors, and in- steadusesthePythondictionaryrepresentationthatwasintroducedindocdist4.count frequency already used that representation internally, so we only need to remove the code that converted the Python dictionary to a list.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return DThis method still takes O(W ) time to process a W -word document.word frequencies for file does not call merge sort anymore, and instead re- turnsthedictionarybuiltbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingNextup,inner productmakesusesdictionarylookupsinsteadofmergingsortedlists.def inner_product(D1,D2): sum = 0.0for key in D1:if key in D2:sum += D1[key] * D2[key]return sum
6.006 Intro to Algorithms Recitation 2 September 11, 2013The logic is quite similar to the straightforward inner product in docdist1. Each word in the first document vector is looked up in the second document vector. However, because the document vectors are dictionaries, each takes O(1) time, and inner product runs in O(L1) time, where L1 is the length of the first document’s vector.docdist7 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 910docdist8At this point, all the algorithms in our solution are asymptotically optimal. We can easily prove this, by noting that each of the 3 main operations runs in time proportional to its input size, and each operation needs to read all its input to produce its output. However, there is still room for optimizing and simplifying the code.There is no reason to read each document line by line, and then break up each line into words. The last iteration processes reads each document into one large string, and breaks up the entire document into words at once.Firstoff,read fileismodifiedtoreturnasinglestring,insteadofanarrayofstrings.Then,get words from line listisrenamedtoget words from file,andissimplified, becauseitdoesn’tneedtoiterateoveralistoflinesanymore.Last,word frequencies for file is updated to reflect the renaming.def get_words_from_string(string):string = string.translate(translation_table) word_list = string.split()return word_listdef word_frequencies_for_file(filename): text = read_file(filename)word_list = get_words_from_string(text) freq_mapping = count_frequency(word_list) return freq_mapping
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist8 Performance ScorecardMethodTimeget words from textcount frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 9101 2 3 4 51 2 3 4 5 6 7 86.006 Intro to Algorithms Recitation 2 September 11, 2013Document Distancedocdist1.py is a straightforward solution to the document distance problem, and docdist{2-8}.py show algorithmic optimizations that improve the running time.We start out by understanding the structure of the straightforward implementation, and then we’ll look at the changes in each of the successive versions.docdist1We first look at main to get a high-level view of what’s going on in the program.def main():if len(sys.argv) != 3:print "Usage: docdist1.py filename_1 filename_2" else:filename_1 = sys.argv[1]filename_2 = sys.argv[2]document_vector_1 = word_frequencies_for_file(filename_1) document_vector_2 = word_frequencies_for_file(filename_2)distance = vector_angle(document_vector_1,document_vector_2)print "The distance between the documents is: %0.6f (radians)"%distanceThemethodprocessesthecommand-linearguments,andcallsword frequencies for file for each document, then calls vector angle on the resulting lists. How do these methods matchupwiththethreeoperationsoutlinedinlecture?Itseemslikeword frequencies for file is responsible for operation 1 (split each document into words) and operation 2 (count word fre- quencies),andthenvector angleisresponsibleforoperation3(computedotproduct).Nextup,we’lltakealookatword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingThe method first calls read file, which returns a list of lines in the input file. We’ll omit the method code, because it is not particularly interesting, and we’ll assume that read file’s running time is proportional to the size of the input file. The input from read line is given to get words from line list, which computes operation 1 (split each document into words).Afterthat,count frequencyturnsthelistofwordsintoadocumentvector(operation 2).def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list = word_list + words_in_line return word_listdef get_words_from_string(line):
 91011121314151617181920212223elif len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word) character_list = []if len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word)return word_list1 2 3 4 5 6 7 8 9101 2 3get words from string takes one line in the input file and breaks it up into a list of words. We iterate through the for loop O(k) times where k is the length of the line. All of the operations within the loop take constant time to run, with the exception of join. Although the join operator takes O(c) time on an input of c characters, each character in the line is joined only once.Therefore,get words from stringhasanoverallruntimeofO(k).get words from line list calls get words from string for each line andcombines the lists into one big list. Line 5 looks innocent but is a big performance killer, becauseusing + to combine W lists of length k is O(W2 ). kkThe output of get words from line list is a list of words, like [’a’, ’cat’,’in’, ’a’, ’bag’].word frequencies from filepassesthisoutputtocount frequency, which turns it into a document vector that counts the number of occurrences of each word, and lookslike[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]].def count_frequency(word_list): L=[]for new_word in word_list: for entry in L:if new_word == entry[0]: entry[1] = entry[1] + 1 breakelse: L.append([new_word,1])return LThe implementation above builds the document vector by takes each word in the input list and looking it up in the list representing the under-construction document vector. In the worst case of a document with all different words, this takes O(W 2 × l) time, where W is the number of words in the document, and l is the average word length.count frequency is the last function call in word frequencies for file. Next up,maincallsvector angle,whichperformsoperation3,computingthedistancemetric.def vector_angle(L1,L2):numerator = inner_product(L1,L2)denominator = math.sqrt(inner_product(L1,L1)*inner_product(L2,L2))6.006 Intro to Algorithmsword_list = [] character_list = [] for c in line:if c.isalnum(): character_list.append(c)Recitation 2September 11, 2013
41 2 3 4 5 6 76.006 Intro to Algorithms Recitation 2 September 11, 2013return math.acos(numerator/denominator)The method is a somewhat straightforward implementation of the distance metric􏰀 L1 · L2 􏰁 􏰄 L1 · L2 􏰅 arccos |L1||L2| = arccos 􏰇(L1 · L1)(L2 · L2),anddelegatestoinner productforthehardworkofcomputingcrossproducts.def inner_product(L1,L2): sum = 0.0for word1, count1 in L1:for word2, count2 in L2: if word1 == word2:sum += count1 * count2 return suminner product is a straightforward inner-product implementation that checks each each word in the first list against the entire second list. The nested loops at lines 3 and 4 give the algorithm its running time of Θ(L1L2), where L1 and L2 are the lengths of the documents’ vectors (the number of unique words in each document).docdist1 Performance ScorecardWe assume that k (number of words per line) is a constant, because the documents will need to fit on screens or paper sheets with a finite length. W (the number of words in a document) is ≥ L (the number of unique words in a document). L21 + L2 + L1L2 = O(L21 + L2) because L21 + L2 ≥ L1L2. Proof (assuming L1, L2 ≥ 0):MethodTimeget words from line list count frequencyword frequencies for fileO(W2 ) = O(W2) kO(WL) O(W2)inner productvector angleO(L1L2) O(L1L2 + L21 + L2) = O(L21 + L2)mainO ( W 12 + W 2 2 )docdist2(L1−L2)2 ≥0 L21+L2−2L1L2 ≥ 0L21+L2 ≥ 2L1L2 L 21 + L 2 2 ≥ L 1 L 2The document distance code invokes the Python profiler to identify the code that takes up the most CPU time. This ensures that we get the biggest returns on our optimization efforts.
1 2 36.006 Intro to Algorithms Recitation 2 September 11, 2013if __name__ == "__main__": import cProfile cProfile.run("main()")You can profile existing programs without changing them by adding -m cProfile -s Time to Python’s command line. For example, the command below will run and profile program.py.  python -m cProfile -s time program.pyA sample profiler output is shown below.123456 ncalls tottime 7 1 0.001 8 2 0.000 9 3 6.25510 1 0.000 11 1 0.004 12 2 0.000 13 2 16.995 14 22663 0.793 15 2 11.205 16 325087 0.034 17 1 0.000 18 1 0.000 19 1241849 0.145 20 1 0.000 21 1300248 0.112 22 232140 0.054 23 232140 0.035 24 2 0.003 25 2 0.000The distance between the documents is: 0.574160 (radians) 3354148 function calls in 35.636 CPU secondsOrdered by: standard namepercall  cumtime  0.001   35.636  0.000   29.376  2.085    6.255  0.000    6.255  0.004   35.635  0.000    0.003  8.498   18.167  0.000    1.172  5.602   11.206  0.000    0.034  0.000    0.000  0.000    0.000  0.000    0.145  0.000    0.000  0.000    0.112  0.000    0.0540.000 0.0350.001 0.0030.000 0.000percall filename:lineno(function) 35.636 <string>:1(<module>) 14.688 docdist1.py:100(word_frequencies_for_file)  2.085 docdist1.py:110(inner_product)  6.255 docdist1.py:125(vector_angle) 35.635 docdist1.py:135(main)  0.001 docdist1.py:37(read_file)  9.083 docdist1.py:49(get_words_from_line_list)  0.000 docdist1.py:61(get_words_from_string)  5.603 docdist1.py:86(count_frequency)  0.000 {len}  0.000 {math.acos}  0.000 {math.sqrt}  0.000 {method ’append’ of ’list’ objects}  0.000 {method ’disable’ of ’_lsprof.Profiler’ objec  0.000 {method ’isalnum’ of ’str’ objects}  0.000 {method ’join’ of ’str’ objects}  0.000 {method ’lower’ of ’str’ objects}  0.001 {method ’readlines’ of ’file’ objects}  0.000 {open}1 2 3 4 5 6Theprofileroutputfordocdist1showsthatthebiggesttimedrainisget words from line list. The problem is that when the + operator is used to concatenate lists, it needs to create a new listand copy the elements of both its operands. Replacing + with extend yields a 30% runtime improvement.def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list.extend(words_in_line) return word_listextend adds all the elements of an m-element list to an n-element list in Θ(1+m), as opposed to +, which needs to create a new list, and therefore takes Θ(1 + n + m) time. So concatenatingkW lists of k elements takes 􏰆W k = Θ(W) time.k
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist2 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(WL) O(WL)inner productvector angleO(L1L2) O ( L 21 + L 2 2 )mainO(W1L1 + W2L2)1 2 3 4 5 61 2 3 4 5 6 7 8 91 2 3 4 5 6 7docdist3Profilingdocdist2pointstocount frequencyandinner productasgoodtargetsforop- timizations. We’ll speed up inner product by switching to a fast algorithm. However, the algorithmassumesthatthewordsinthedocumentvectorsaresorted.Forexample,[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]]needstobecome[[’a’, 2], [’bag’, 1], [’cat’, 1], [’in’, 1]].Firstoff,weaddasteptoword frequencies for filethatsortsthedocumentvector producedbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) insertion_sort(freq_mapping)return freq_mappingThenweimplementinsertion sortusingthealgorithminthetextbook.def insertion_sort(A): for j in range(len(A)):key = A[j]i = j-1while i>-1 and A[i]>key:A[i+1] = A[i]      i = i-1    A[i+1] = keyreturn AInsertion sort runs in O(L2) time, where L is the length of the document vector to be sorted.Finally,inner productisre-implementedusingasimilaralgorithmtothemergingstepin Merge Sort.def inner_product(L1,L2): sum = 0.0i=0j=0while i<len(L1) and j<len(L2):# L1[i:] and L2[j:] yet to be processed if L1[i][0] == L2[j][0]:
8 91011121314151617186.006 Intro to Algorithms Recitation 2 September 11, 2013      # both vectors have this word      sum += L1[i][1] * L2[j][1]      i += 1      j += 1elif L1[i][0] < L2[j][0]:# word L1[i][0] is in L1 but not L2 i += 1else:# word L2[j][0] is in L2 but not L1 j += 1return sumThe new implementation runs in Θ(L1 + L2), where L1 and L2 are the lengths of the two document vectors. We observe that the running time for inner product (and therefore for vector angle)isasymptoticallyoptimal,becauseanyalgorithmthatcomputestheinnerprod- uct will have to read the two vectors, and reading will take Ω(L1 + L2) time.docdist3 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(WL)O(L2) O(WL + L2) = O(WL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1L1 + W2L2)1 2 3 4 5 6 7 8docdist4The next iteration addresses count frequency, which is the biggest time consumer at the moment.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return D.items()The new implementation uses Python dictionaries. The dictionaries are implemented using hash tables, which will be presented in a future lecture. The salient feature of hash tables is that inserting an element using dictionary[key] = value and looking up an element using dictionary[key] both run in O(1) expected time.Instead of storing the document vector under construction in a list, the new implementation uses a dictionary. The keys are the words in the document, and the value are the number of times
6.006 Intro to Algorithms Recitation 2 September 11, 2013 each word appears in the document. Since both insertion (line 5) and lookup (line 7) take O(1)time, building a document vector out of W words takes O(W ) time. docdist4 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(W)O(L2) O(W + L2) = O(L2)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L21 +L2)1 2 3 4 5 6 7docdist5Thisiterationsimplifiesget words from stringthatbreaksuplinesintowords.Firstoff, the standard library function string.translate is used for converting uppercase characters to lowercase, and for converting punctuation to spaces. Second, the split method on strings is used to break up a line into words.translation_table = string.maketrans(string.punctuation+string.uppercase,                   " "*len(string.punctuation)+string.lowercase)def get_words_from_string(line):line = line.translate(translation_table) word_list = line.split()return word_listThe main benefit of this change is that 23 lines of code are replaced with 5 lines of code. This makes the implementation easier to analyze. A side benefit is that many functions in the Python standard library are implemented in directly in C (a low-level programming language that is very close to machine code), which gives them better performance. Although the running time is asymptotically the same, the hidden constants are much better for the C code than for our Python code presented in docdist1.docdist5 Performance ScorecardIdentical to the docdist4 scorecard.docdist6Nowthatallthedistractionsareoutoftheway,it’stimetotackleinsertion sort,whichis takes up the most CPU time, by far, in the profiler output for docdist5.The advantages of insertion sort are that it sorts in place, and it is simple to implement. However, its worst-case running time is O(N2) for an N-element array. We’ll replace insertion
1 2 3 4 5 61 2 3 4 5 6 7 8 9101112131415161718192021222324256.006 Intro to Algorithms Recitation 2 September 11, 2013 sort with a better algorithm, merge sort. Merge sort is not in place, so we’ll need to modifyword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) freq_mapping = merge_sort(freq_mapping)return freq_mappingThemerge sortimplementationcloselyfollowsthepseudocodeinthetextbook.def merge_sort(A): n = len(A)if n==1:return Amid = n//2L = merge_sort(A[:mid]) R = merge_sort(A[mid:]) return merge(L,R)def merge(L,R): i=0j=0answer = []while i<len(L) and j<len(R):if L[i]<R[j]: answer.append(L[i]) i += 1else: answer.append(R[j]) j += 1if i<len(L): answer.extend(L[i:])if j<len(R): answer.extend(R[j:])return answerThe textbook proves that merge sort runs in Θ(n log n) time. You should apply your knowledge of the Python cost model to convince yourself that the implementation above also runs in Θ(n log n) time.
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist6 Performance ScorecardMethodTimeget words from line list count frequencymerge sortword frequencies for fileO(W)O(W) O(L log L) O(W +LlogL)=O(LlogL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L1 logL1 +L2 logL2)1 2 3 4 5 6 7 81 2 3 4 51 2 3 4 5 6docdist7Switching to merge sort improved the running time dramatically. However, if we look at docdist6’s profiler output, we notice that merge is the function with the biggest runtime footprint. Merge sort’s performance in practice is great, so it seems that the only way to make our code faster is to get rid of sorting altogether.This iteration switches away from the sorted list representation of document vectors, and in- steadusesthePythondictionaryrepresentationthatwasintroducedindocdist4.count frequency already used that representation internally, so we only need to remove the code that converted the Python dictionary to a list.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return DThis method still takes O(W ) time to process a W -word document.word frequencies for file does not call merge sort anymore, and instead re- turnsthedictionarybuiltbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingNextup,inner productmakesusesdictionarylookupsinsteadofmergingsortedlists.def inner_product(D1,D2): sum = 0.0for key in D1:if key in D2:sum += D1[key] * D2[key]return sum
6.006 Intro to Algorithms Recitation 2 September 11, 2013The logic is quite similar to the straightforward inner product in docdist1. Each word in the first document vector is looked up in the second document vector. However, because the document vectors are dictionaries, each takes O(1) time, and inner product runs in O(L1) time, where L1 is the length of the first document’s vector.docdist7 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 910docdist8At this point, all the algorithms in our solution are asymptotically optimal. We can easily prove this, by noting that each of the 3 main operations runs in time proportional to its input size, and each operation needs to read all its input to produce its output. However, there is still room for optimizing and simplifying the code.There is no reason to read each document line by line, and then break up each line into words. The last iteration processes reads each document into one large string, and breaks up the entire document into words at once.Firstoff,read fileismodifiedtoreturnasinglestring,insteadofanarrayofstrings.Then,get words from line listisrenamedtoget words from file,andissimplified, becauseitdoesn’tneedtoiterateoveralistoflinesanymore.Last,word frequencies for file is updated to reflect the renaming.def get_words_from_string(string):string = string.translate(translation_table) word_list = string.split()return word_listdef word_frequencies_for_file(filename): text = read_file(filename)word_list = get_words_from_string(text) freq_mapping = count_frequency(word_list) return freq_mapping
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist8 Performance ScorecardMethodTimeget words from textcount frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 9101 2 3 4 51 2 3 4 5 6 7 86.006 Intro to Algorithms Recitation 2 September 11, 2013Document Distancedocdist1.py is a straightforward solution to the document distance problem, and docdist{2-8}.py show algorithmic optimizations that improve the running time.We start out by understanding the structure of the straightforward implementation, and then we’ll look at the changes in each of the successive versions.docdist1We first look at main to get a high-level view of what’s going on in the program.def main():if len(sys.argv) != 3:print "Usage: docdist1.py filename_1 filename_2" else:filename_1 = sys.argv[1]filename_2 = sys.argv[2]document_vector_1 = word_frequencies_for_file(filename_1) document_vector_2 = word_frequencies_for_file(filename_2)distance = vector_angle(document_vector_1,document_vector_2)print "The distance between the documents is: %0.6f (radians)"%distanceThemethodprocessesthecommand-linearguments,andcallsword frequencies for file for each document, then calls vector angle on the resulting lists. How do these methods matchupwiththethreeoperationsoutlinedinlecture?Itseemslikeword frequencies for file is responsible for operation 1 (split each document into words) and operation 2 (count word fre- quencies),andthenvector angleisresponsibleforoperation3(computedotproduct).Nextup,we’lltakealookatword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingThe method first calls read file, which returns a list of lines in the input file. We’ll omit the method code, because it is not particularly interesting, and we’ll assume that read file’s running time is proportional to the size of the input file. The input from read line is given to get words from line list, which computes operation 1 (split each document into words).Afterthat,count frequencyturnsthelistofwordsintoadocumentvector(operation 2).def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list = word_list + words_in_line return word_listdef get_words_from_string(line):
 91011121314151617181920212223elif len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word) character_list = []if len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word)return word_list1 2 3 4 5 6 7 8 9101 2 3get words from string takes one line in the input file and breaks it up into a list of words. We iterate through the for loop O(k) times where k is the length of the line. All of the operations within the loop take constant time to run, with the exception of join. Although the join operator takes O(c) time on an input of c characters, each character in the line is joined only once.Therefore,get words from stringhasanoverallruntimeofO(k).get words from line list calls get words from string for each line andcombines the lists into one big list. Line 5 looks innocent but is a big performance killer, becauseusing + to combine W lists of length k is O(W2 ). kkThe output of get words from line list is a list of words, like [’a’, ’cat’,’in’, ’a’, ’bag’].word frequencies from filepassesthisoutputtocount frequency, which turns it into a document vector that counts the number of occurrences of each word, and lookslike[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]].def count_frequency(word_list): L=[]for new_word in word_list: for entry in L:if new_word == entry[0]: entry[1] = entry[1] + 1 breakelse: L.append([new_word,1])return LThe implementation above builds the document vector by takes each word in the input list and looking it up in the list representing the under-construction document vector. In the worst case of a document with all different words, this takes O(W 2 × l) time, where W is the number of words in the document, and l is the average word length.count frequency is the last function call in word frequencies for file. Next up,maincallsvector angle,whichperformsoperation3,computingthedistancemetric.def vector_angle(L1,L2):numerator = inner_product(L1,L2)denominator = math.sqrt(inner_product(L1,L1)*inner_product(L2,L2))6.006 Intro to Algorithmsword_list = [] character_list = [] for c in line:if c.isalnum(): character_list.append(c)Recitation 2September 11, 2013
41 2 3 4 5 6 76.006 Intro to Algorithms Recitation 2 September 11, 2013return math.acos(numerator/denominator)The method is a somewhat straightforward implementation of the distance metric􏰀 L1 · L2 􏰁 􏰄 L1 · L2 􏰅 arccos |L1||L2| = arccos 􏰇(L1 · L1)(L2 · L2),anddelegatestoinner productforthehardworkofcomputingcrossproducts.def inner_product(L1,L2): sum = 0.0for word1, count1 in L1:for word2, count2 in L2: if word1 == word2:sum += count1 * count2 return suminner product is a straightforward inner-product implementation that checks each each word in the first list against the entire second list. The nested loops at lines 3 and 4 give the algorithm its running time of Θ(L1L2), where L1 and L2 are the lengths of the documents’ vectors (the number of unique words in each document).docdist1 Performance ScorecardWe assume that k (number of words per line) is a constant, because the documents will need to fit on screens or paper sheets with a finite length. W (the number of words in a document) is ≥ L (the number of unique words in a document). L21 + L2 + L1L2 = O(L21 + L2) because L21 + L2 ≥ L1L2. Proof (assuming L1, L2 ≥ 0):MethodTimeget words from line list count frequencyword frequencies for fileO(W2 ) = O(W2) kO(WL) O(W2)inner productvector angleO(L1L2) O(L1L2 + L21 + L2) = O(L21 + L2)mainO ( W 12 + W 2 2 )docdist2(L1−L2)2 ≥0 L21+L2−2L1L2 ≥ 0L21+L2 ≥ 2L1L2 L 21 + L 2 2 ≥ L 1 L 2The document distance code invokes the Python profiler to identify the code that takes up the most CPU time. This ensures that we get the biggest returns on our optimization efforts.
1 2 36.006 Intro to Algorithms Recitation 2 September 11, 2013if __name__ == "__main__": import cProfile cProfile.run("main()")You can profile existing programs without changing them by adding -m cProfile -s Time to Python’s command line. For example, the command below will run and profile program.py.  python -m cProfile -s time program.pyA sample profiler output is shown below.123456 ncalls tottime 7 1 0.001 8 2 0.000 9 3 6.25510 1 0.000 11 1 0.004 12 2 0.000 13 2 16.995 14 22663 0.793 15 2 11.205 16 325087 0.034 17 1 0.000 18 1 0.000 19 1241849 0.145 20 1 0.000 21 1300248 0.112 22 232140 0.054 23 232140 0.035 24 2 0.003 25 2 0.000The distance between the documents is: 0.574160 (radians) 3354148 function calls in 35.636 CPU secondsOrdered by: standard namepercall  cumtime  0.001   35.636  0.000   29.376  2.085    6.255  0.000    6.255  0.004   35.635  0.000    0.003  8.498   18.167  0.000    1.172  5.602   11.206  0.000    0.034  0.000    0.000  0.000    0.000  0.000    0.145  0.000    0.000  0.000    0.112  0.000    0.0540.000 0.0350.001 0.0030.000 0.000percall filename:lineno(function) 35.636 <string>:1(<module>) 14.688 docdist1.py:100(word_frequencies_for_file)  2.085 docdist1.py:110(inner_product)  6.255 docdist1.py:125(vector_angle) 35.635 docdist1.py:135(main)  0.001 docdist1.py:37(read_file)  9.083 docdist1.py:49(get_words_from_line_list)  0.000 docdist1.py:61(get_words_from_string)  5.603 docdist1.py:86(count_frequency)  0.000 {len}  0.000 {math.acos}  0.000 {math.sqrt}  0.000 {method ’append’ of ’list’ objects}  0.000 {method ’disable’ of ’_lsprof.Profiler’ objec  0.000 {method ’isalnum’ of ’str’ objects}  0.000 {method ’join’ of ’str’ objects}  0.000 {method ’lower’ of ’str’ objects}  0.001 {method ’readlines’ of ’file’ objects}  0.000 {open}1 2 3 4 5 6Theprofileroutputfordocdist1showsthatthebiggesttimedrainisget words from line list. The problem is that when the + operator is used to concatenate lists, it needs to create a new listand copy the elements of both its operands. Replacing + with extend yields a 30% runtime improvement.def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list.extend(words_in_line) return word_listextend adds all the elements of an m-element list to an n-element list in Θ(1+m), as opposed to +, which needs to create a new list, and therefore takes Θ(1 + n + m) time. So concatenatingkW lists of k elements takes 􏰆W k = Θ(W) time.k
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist2 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(WL) O(WL)inner productvector angleO(L1L2) O ( L 21 + L 2 2 )mainO(W1L1 + W2L2)1 2 3 4 5 61 2 3 4 5 6 7 8 91 2 3 4 5 6 7docdist3Profilingdocdist2pointstocount frequencyandinner productasgoodtargetsforop- timizations. We’ll speed up inner product by switching to a fast algorithm. However, the algorithmassumesthatthewordsinthedocumentvectorsaresorted.Forexample,[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]]needstobecome[[’a’, 2], [’bag’, 1], [’cat’, 1], [’in’, 1]].Firstoff,weaddasteptoword frequencies for filethatsortsthedocumentvector producedbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) insertion_sort(freq_mapping)return freq_mappingThenweimplementinsertion sortusingthealgorithminthetextbook.def insertion_sort(A): for j in range(len(A)):key = A[j]i = j-1while i>-1 and A[i]>key:A[i+1] = A[i]      i = i-1    A[i+1] = keyreturn AInsertion sort runs in O(L2) time, where L is the length of the document vector to be sorted.Finally,inner productisre-implementedusingasimilaralgorithmtothemergingstepin Merge Sort.def inner_product(L1,L2): sum = 0.0i=0j=0while i<len(L1) and j<len(L2):# L1[i:] and L2[j:] yet to be processed if L1[i][0] == L2[j][0]:
8 91011121314151617186.006 Intro to Algorithms Recitation 2 September 11, 2013      # both vectors have this word      sum += L1[i][1] * L2[j][1]      i += 1      j += 1elif L1[i][0] < L2[j][0]:# word L1[i][0] is in L1 but not L2 i += 1else:# word L2[j][0] is in L2 but not L1 j += 1return sumThe new implementation runs in Θ(L1 + L2), where L1 and L2 are the lengths of the two document vectors. We observe that the running time for inner product (and therefore for vector angle)isasymptoticallyoptimal,becauseanyalgorithmthatcomputestheinnerprod- uct will have to read the two vectors, and reading will take Ω(L1 + L2) time.docdist3 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(WL)O(L2) O(WL + L2) = O(WL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1L1 + W2L2)1 2 3 4 5 6 7 8docdist4The next iteration addresses count frequency, which is the biggest time consumer at the moment.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return D.items()The new implementation uses Python dictionaries. The dictionaries are implemented using hash tables, which will be presented in a future lecture. The salient feature of hash tables is that inserting an element using dictionary[key] = value and looking up an element using dictionary[key] both run in O(1) expected time.Instead of storing the document vector under construction in a list, the new implementation uses a dictionary. The keys are the words in the document, and the value are the number of times
6.006 Intro to Algorithms Recitation 2 September 11, 2013 each word appears in the document. Since both insertion (line 5) and lookup (line 7) take O(1)time, building a document vector out of W words takes O(W ) time. docdist4 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(W)O(L2) O(W + L2) = O(L2)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L21 +L2)1 2 3 4 5 6 7docdist5Thisiterationsimplifiesget words from stringthatbreaksuplinesintowords.Firstoff, the standard library function string.translate is used for converting uppercase characters to lowercase, and for converting punctuation to spaces. Second, the split method on strings is used to break up a line into words.translation_table = string.maketrans(string.punctuation+string.uppercase,                   " "*len(string.punctuation)+string.lowercase)def get_words_from_string(line):line = line.translate(translation_table) word_list = line.split()return word_listThe main benefit of this change is that 23 lines of code are replaced with 5 lines of code. This makes the implementation easier to analyze. A side benefit is that many functions in the Python standard library are implemented in directly in C (a low-level programming language that is very close to machine code), which gives them better performance. Although the running time is asymptotically the same, the hidden constants are much better for the C code than for our Python code presented in docdist1.docdist5 Performance ScorecardIdentical to the docdist4 scorecard.docdist6Nowthatallthedistractionsareoutoftheway,it’stimetotackleinsertion sort,whichis takes up the most CPU time, by far, in the profiler output for docdist5.The advantages of insertion sort are that it sorts in place, and it is simple to implement. However, its worst-case running time is O(N2) for an N-element array. We’ll replace insertion
1 2 3 4 5 61 2 3 4 5 6 7 8 9101112131415161718192021222324256.006 Intro to Algorithms Recitation 2 September 11, 2013 sort with a better algorithm, merge sort. Merge sort is not in place, so we’ll need to modifyword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) freq_mapping = merge_sort(freq_mapping)return freq_mappingThemerge sortimplementationcloselyfollowsthepseudocodeinthetextbook.def merge_sort(A): n = len(A)if n==1:return Amid = n//2L = merge_sort(A[:mid]) R = merge_sort(A[mid:]) return merge(L,R)def merge(L,R): i=0j=0answer = []while i<len(L) and j<len(R):if L[i]<R[j]: answer.append(L[i]) i += 1else: answer.append(R[j]) j += 1if i<len(L): answer.extend(L[i:])if j<len(R): answer.extend(R[j:])return answerThe textbook proves that merge sort runs in Θ(n log n) time. You should apply your knowledge of the Python cost model to convince yourself that the implementation above also runs in Θ(n log n) time.
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist6 Performance ScorecardMethodTimeget words from line list count frequencymerge sortword frequencies for fileO(W)O(W) O(L log L) O(W +LlogL)=O(LlogL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L1 logL1 +L2 logL2)1 2 3 4 5 6 7 81 2 3 4 51 2 3 4 5 6docdist7Switching to merge sort improved the running time dramatically. However, if we look at docdist6’s profiler output, we notice that merge is the function with the biggest runtime footprint. Merge sort’s performance in practice is great, so it seems that the only way to make our code faster is to get rid of sorting altogether.This iteration switches away from the sorted list representation of document vectors, and in- steadusesthePythondictionaryrepresentationthatwasintroducedindocdist4.count frequency already used that representation internally, so we only need to remove the code that converted the Python dictionary to a list.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return DThis method still takes O(W ) time to process a W -word document.word frequencies for file does not call merge sort anymore, and instead re- turnsthedictionarybuiltbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingNextup,inner productmakesusesdictionarylookupsinsteadofmergingsortedlists.def inner_product(D1,D2): sum = 0.0for key in D1:if key in D2:sum += D1[key] * D2[key]return sum
6.006 Intro to Algorithms Recitation 2 September 11, 2013The logic is quite similar to the straightforward inner product in docdist1. Each word in the first document vector is looked up in the second document vector. However, because the document vectors are dictionaries, each takes O(1) time, and inner product runs in O(L1) time, where L1 is the length of the first document’s vector.docdist7 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 910docdist8At this point, all the algorithms in our solution are asymptotically optimal. We can easily prove this, by noting that each of the 3 main operations runs in time proportional to its input size, and each operation needs to read all its input to produce its output. However, there is still room for optimizing and simplifying the code.There is no reason to read each document line by line, and then break up each line into words. The last iteration processes reads each document into one large string, and breaks up the entire document into words at once.Firstoff,read fileismodifiedtoreturnasinglestring,insteadofanarrayofstrings.Then,get words from line listisrenamedtoget words from file,andissimplified, becauseitdoesn’tneedtoiterateoveralistoflinesanymore.Last,word frequencies for file is updated to reflect the renaming.def get_words_from_string(string):string = string.translate(translation_table) word_list = string.split()return word_listdef word_frequencies_for_file(filename): text = read_file(filename)word_list = get_words_from_string(text) freq_mapping = count_frequency(word_list) return freq_mapping
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist8 Performance ScorecardMethodTimeget words from textcount frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 9101 2 3 4 51 2 3 4 5 6 7 86.006 Intro to Algorithms Recitation 2 September 11, 2013Document Distancedocdist1.py is a straightforward solution to the document distance problem, and docdist{2-8}.py show algorithmic optimizations that improve the running time.We start out by understanding the structure of the straightforward implementation, and then we’ll look at the changes in each of the successive versions.docdist1We first look at main to get a high-level view of what’s going on in the program.def main():if len(sys.argv) != 3:print "Usage: docdist1.py filename_1 filename_2" else:filename_1 = sys.argv[1]filename_2 = sys.argv[2]document_vector_1 = word_frequencies_for_file(filename_1) document_vector_2 = word_frequencies_for_file(filename_2)distance = vector_angle(document_vector_1,document_vector_2)print "The distance between the documents is: %0.6f (radians)"%distanceThemethodprocessesthecommand-linearguments,andcallsword frequencies for file for each document, then calls vector angle on the resulting lists. How do these methods matchupwiththethreeoperationsoutlinedinlecture?Itseemslikeword frequencies for file is responsible for operation 1 (split each document into words) and operation 2 (count word fre- quencies),andthenvector angleisresponsibleforoperation3(computedotproduct).Nextup,we’lltakealookatword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingThe method first calls read file, which returns a list of lines in the input file. We’ll omit the method code, because it is not particularly interesting, and we’ll assume that read file’s running time is proportional to the size of the input file. The input from read line is given to get words from line list, which computes operation 1 (split each document into words).Afterthat,count frequencyturnsthelistofwordsintoadocumentvector(operation 2).def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list = word_list + words_in_line return word_listdef get_words_from_string(line):
 91011121314151617181920212223elif len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word) character_list = []if len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word)return word_list1 2 3 4 5 6 7 8 9101 2 3get words from string takes one line in the input file and breaks it up into a list of words. We iterate through the for loop O(k) times where k is the length of the line. All of the operations within the loop take constant time to run, with the exception of join. Although the join operator takes O(c) time on an input of c characters, each character in the line is joined only once.Therefore,get words from stringhasanoverallruntimeofO(k).get words from line list calls get words from string for each line andcombines the lists into one big list. Line 5 looks innocent but is a big performance killer, becauseusing + to combine W lists of length k is O(W2 ). kkThe output of get words from line list is a list of words, like [’a’, ’cat’,’in’, ’a’, ’bag’].word frequencies from filepassesthisoutputtocount frequency, which turns it into a document vector that counts the number of occurrences of each word, and lookslike[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]].def count_frequency(word_list): L=[]for new_word in word_list: for entry in L:if new_word == entry[0]: entry[1] = entry[1] + 1 breakelse: L.append([new_word,1])return LThe implementation above builds the document vector by takes each word in the input list and looking it up in the list representing the under-construction document vector. In the worst case of a document with all different words, this takes O(W 2 × l) time, where W is the number of words in the document, and l is the average word length.count frequency is the last function call in word frequencies for file. Next up,maincallsvector angle,whichperformsoperation3,computingthedistancemetric.def vector_angle(L1,L2):numerator = inner_product(L1,L2)denominator = math.sqrt(inner_product(L1,L1)*inner_product(L2,L2))6.006 Intro to Algorithmsword_list = [] character_list = [] for c in line:if c.isalnum(): character_list.append(c)Recitation 2September 11, 2013
41 2 3 4 5 6 76.006 Intro to Algorithms Recitation 2 September 11, 2013return math.acos(numerator/denominator)The method is a somewhat straightforward implementation of the distance metric􏰀 L1 · L2 􏰁 􏰄 L1 · L2 􏰅 arccos |L1||L2| = arccos 􏰇(L1 · L1)(L2 · L2),anddelegatestoinner productforthehardworkofcomputingcrossproducts.def inner_product(L1,L2): sum = 0.0for word1, count1 in L1:for word2, count2 in L2: if word1 == word2:sum += count1 * count2 return suminner product is a straightforward inner-product implementation that checks each each word in the first list against the entire second list. The nested loops at lines 3 and 4 give the algorithm its running time of Θ(L1L2), where L1 and L2 are the lengths of the documents’ vectors (the number of unique words in each document).docdist1 Performance ScorecardWe assume that k (number of words per line) is a constant, because the documents will need to fit on screens or paper sheets with a finite length. W (the number of words in a document) is ≥ L (the number of unique words in a document). L21 + L2 + L1L2 = O(L21 + L2) because L21 + L2 ≥ L1L2. Proof (assuming L1, L2 ≥ 0):MethodTimeget words from line list count frequencyword frequencies for fileO(W2 ) = O(W2) kO(WL) O(W2)inner productvector angleO(L1L2) O(L1L2 + L21 + L2) = O(L21 + L2)mainO ( W 12 + W 2 2 )docdist2(L1−L2)2 ≥0 L21+L2−2L1L2 ≥ 0L21+L2 ≥ 2L1L2 L 21 + L 2 2 ≥ L 1 L 2The document distance code invokes the Python profiler to identify the code that takes up the most CPU time. This ensures that we get the biggest returns on our optimization efforts.
1 2 36.006 Intro to Algorithms Recitation 2 September 11, 2013if __name__ == "__main__": import cProfile cProfile.run("main()")You can profile existing programs without changing them by adding -m cProfile -s Time to Python’s command line. For example, the command below will run and profile program.py.  python -m cProfile -s time program.pyA sample profiler output is shown below.123456 ncalls tottime 7 1 0.001 8 2 0.000 9 3 6.25510 1 0.000 11 1 0.004 12 2 0.000 13 2 16.995 14 22663 0.793 15 2 11.205 16 325087 0.034 17 1 0.000 18 1 0.000 19 1241849 0.145 20 1 0.000 21 1300248 0.112 22 232140 0.054 23 232140 0.035 24 2 0.003 25 2 0.000The distance between the documents is: 0.574160 (radians) 3354148 function calls in 35.636 CPU secondsOrdered by: standard namepercall  cumtime  0.001   35.636  0.000   29.376  2.085    6.255  0.000    6.255  0.004   35.635  0.000    0.003  8.498   18.167  0.000    1.172  5.602   11.206  0.000    0.034  0.000    0.000  0.000    0.000  0.000    0.145  0.000    0.000  0.000    0.112  0.000    0.0540.000 0.0350.001 0.0030.000 0.000percall filename:lineno(function) 35.636 <string>:1(<module>) 14.688 docdist1.py:100(word_frequencies_for_file)  2.085 docdist1.py:110(inner_product)  6.255 docdist1.py:125(vector_angle) 35.635 docdist1.py:135(main)  0.001 docdist1.py:37(read_file)  9.083 docdist1.py:49(get_words_from_line_list)  0.000 docdist1.py:61(get_words_from_string)  5.603 docdist1.py:86(count_frequency)  0.000 {len}  0.000 {math.acos}  0.000 {math.sqrt}  0.000 {method ’append’ of ’list’ objects}  0.000 {method ’disable’ of ’_lsprof.Profiler’ objec  0.000 {method ’isalnum’ of ’str’ objects}  0.000 {method ’join’ of ’str’ objects}  0.000 {method ’lower’ of ’str’ objects}  0.001 {method ’readlines’ of ’file’ objects}  0.000 {open}1 2 3 4 5 6Theprofileroutputfordocdist1showsthatthebiggesttimedrainisget words from line list. The problem is that when the + operator is used to concatenate lists, it needs to create a new listand copy the elements of both its operands. Replacing + with extend yields a 30% runtime improvement.def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list.extend(words_in_line) return word_listextend adds all the elements of an m-element list to an n-element list in Θ(1+m), as opposed to +, which needs to create a new list, and therefore takes Θ(1 + n + m) time. So concatenatingkW lists of k elements takes 􏰆W k = Θ(W) time.k
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist2 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(WL) O(WL)inner productvector angleO(L1L2) O ( L 21 + L 2 2 )mainO(W1L1 + W2L2)1 2 3 4 5 61 2 3 4 5 6 7 8 91 2 3 4 5 6 7docdist3Profilingdocdist2pointstocount frequencyandinner productasgoodtargetsforop- timizations. We’ll speed up inner product by switching to a fast algorithm. However, the algorithmassumesthatthewordsinthedocumentvectorsaresorted.Forexample,[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]]needstobecome[[’a’, 2], [’bag’, 1], [’cat’, 1], [’in’, 1]].Firstoff,weaddasteptoword frequencies for filethatsortsthedocumentvector producedbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) insertion_sort(freq_mapping)return freq_mappingThenweimplementinsertion sortusingthealgorithminthetextbook.def insertion_sort(A): for j in range(len(A)):key = A[j]i = j-1while i>-1 and A[i]>key:A[i+1] = A[i]      i = i-1    A[i+1] = keyreturn AInsertion sort runs in O(L2) time, where L is the length of the document vector to be sorted.Finally,inner productisre-implementedusingasimilaralgorithmtothemergingstepin Merge Sort.def inner_product(L1,L2): sum = 0.0i=0j=0while i<len(L1) and j<len(L2):# L1[i:] and L2[j:] yet to be processed if L1[i][0] == L2[j][0]:
8 91011121314151617186.006 Intro to Algorithms Recitation 2 September 11, 2013      # both vectors have this word      sum += L1[i][1] * L2[j][1]      i += 1      j += 1elif L1[i][0] < L2[j][0]:# word L1[i][0] is in L1 but not L2 i += 1else:# word L2[j][0] is in L2 but not L1 j += 1return sumThe new implementation runs in Θ(L1 + L2), where L1 and L2 are the lengths of the two document vectors. We observe that the running time for inner product (and therefore for vector angle)isasymptoticallyoptimal,becauseanyalgorithmthatcomputestheinnerprod- uct will have to read the two vectors, and reading will take Ω(L1 + L2) time.docdist3 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(WL)O(L2) O(WL + L2) = O(WL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1L1 + W2L2)1 2 3 4 5 6 7 8docdist4The next iteration addresses count frequency, which is the biggest time consumer at the moment.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return D.items()The new implementation uses Python dictionaries. The dictionaries are implemented using hash tables, which will be presented in a future lecture. The salient feature of hash tables is that inserting an element using dictionary[key] = value and looking up an element using dictionary[key] both run in O(1) expected time.Instead of storing the document vector under construction in a list, the new implementation uses a dictionary. The keys are the words in the document, and the value are the number of times
6.006 Intro to Algorithms Recitation 2 September 11, 2013 each word appears in the document. Since both insertion (line 5) and lookup (line 7) take O(1)time, building a document vector out of W words takes O(W ) time. docdist4 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(W)O(L2) O(W + L2) = O(L2)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L21 +L2)1 2 3 4 5 6 7docdist5Thisiterationsimplifiesget words from stringthatbreaksuplinesintowords.Firstoff, the standard library function string.translate is used for converting uppercase characters to lowercase, and for converting punctuation to spaces. Second, the split method on strings is used to break up a line into words.translation_table = string.maketrans(string.punctuation+string.uppercase,                   " "*len(string.punctuation)+string.lowercase)def get_words_from_string(line):line = line.translate(translation_table) word_list = line.split()return word_listThe main benefit of this change is that 23 lines of code are replaced with 5 lines of code. This makes the implementation easier to analyze. A side benefit is that many functions in the Python standard library are implemented in directly in C (a low-level programming language that is very close to machine code), which gives them better performance. Although the running time is asymptotically the same, the hidden constants are much better for the C code than for our Python code presented in docdist1.docdist5 Performance ScorecardIdentical to the docdist4 scorecard.docdist6Nowthatallthedistractionsareoutoftheway,it’stimetotackleinsertion sort,whichis takes up the most CPU time, by far, in the profiler output for docdist5.The advantages of insertion sort are that it sorts in place, and it is simple to implement. However, its worst-case running time is O(N2) for an N-element array. We’ll replace insertion
1 2 3 4 5 61 2 3 4 5 6 7 8 9101112131415161718192021222324256.006 Intro to Algorithms Recitation 2 September 11, 2013 sort with a better algorithm, merge sort. Merge sort is not in place, so we’ll need to modifyword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) freq_mapping = merge_sort(freq_mapping)return freq_mappingThemerge sortimplementationcloselyfollowsthepseudocodeinthetextbook.def merge_sort(A): n = len(A)if n==1:return Amid = n//2L = merge_sort(A[:mid]) R = merge_sort(A[mid:]) return merge(L,R)def merge(L,R): i=0j=0answer = []while i<len(L) and j<len(R):if L[i]<R[j]: answer.append(L[i]) i += 1else: answer.append(R[j]) j += 1if i<len(L): answer.extend(L[i:])if j<len(R): answer.extend(R[j:])return answerThe textbook proves that merge sort runs in Θ(n log n) time. You should apply your knowledge of the Python cost model to convince yourself that the implementation above also runs in Θ(n log n) time.
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist6 Performance ScorecardMethodTimeget words from line list count frequencymerge sortword frequencies for fileO(W)O(W) O(L log L) O(W +LlogL)=O(LlogL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L1 logL1 +L2 logL2)1 2 3 4 5 6 7 81 2 3 4 51 2 3 4 5 6docdist7Switching to merge sort improved the running time dramatically. However, if we look at docdist6’s profiler output, we notice that merge is the function with the biggest runtime footprint. Merge sort’s performance in practice is great, so it seems that the only way to make our code faster is to get rid of sorting altogether.This iteration switches away from the sorted list representation of document vectors, and in- steadusesthePythondictionaryrepresentationthatwasintroducedindocdist4.count frequency already used that representation internally, so we only need to remove the code that converted the Python dictionary to a list.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return DThis method still takes O(W ) time to process a W -word document.word frequencies for file does not call merge sort anymore, and instead re- turnsthedictionarybuiltbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingNextup,inner productmakesusesdictionarylookupsinsteadofmergingsortedlists.def inner_product(D1,D2): sum = 0.0for key in D1:if key in D2:sum += D1[key] * D2[key]return sum
6.006 Intro to Algorithms Recitation 2 September 11, 2013The logic is quite similar to the straightforward inner product in docdist1. Each word in the first document vector is looked up in the second document vector. However, because the document vectors are dictionaries, each takes O(1) time, and inner product runs in O(L1) time, where L1 is the length of the first document’s vector.docdist7 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 910docdist8At this point, all the algorithms in our solution are asymptotically optimal. We can easily prove this, by noting that each of the 3 main operations runs in time proportional to its input size, and each operation needs to read all its input to produce its output. However, there is still room for optimizing and simplifying the code.There is no reason to read each document line by line, and then break up each line into words. The last iteration processes reads each document into one large string, and breaks up the entire document into words at once.Firstoff,read fileismodifiedtoreturnasinglestring,insteadofanarrayofstrings.Then,get words from line listisrenamedtoget words from file,andissimplified, becauseitdoesn’tneedtoiterateoveralistoflinesanymore.Last,word frequencies for file is updated to reflect the renaming.def get_words_from_string(string):string = string.translate(translation_table) word_list = string.split()return word_listdef word_frequencies_for_file(filename): text = read_file(filename)word_list = get_words_from_string(text) freq_mapping = count_frequency(word_list) return freq_mapping
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist8 Performance ScorecardMethodTimeget words from textcount frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 9101 2 3 4 51 2 3 4 5 6 7 86.006 Intro to Algorithms Recitation 2 September 11, 2013Document Distancedocdist1.py is a straightforward solution to the document distance problem, and docdist{2-8}.py show algorithmic optimizations that improve the running time.We start out by understanding the structure of the straightforward implementation, and then we’ll look at the changes in each of the successive versions.docdist1We first look at main to get a high-level view of what’s going on in the program.def main():if len(sys.argv) != 3:print "Usage: docdist1.py filename_1 filename_2" else:filename_1 = sys.argv[1]filename_2 = sys.argv[2]document_vector_1 = word_frequencies_for_file(filename_1) document_vector_2 = word_frequencies_for_file(filename_2)distance = vector_angle(document_vector_1,document_vector_2)print "The distance between the documents is: %0.6f (radians)"%distanceThemethodprocessesthecommand-linearguments,andcallsword frequencies for file for each document, then calls vector angle on the resulting lists. How do these methods matchupwiththethreeoperationsoutlinedinlecture?Itseemslikeword frequencies for file is responsible for operation 1 (split each document into words) and operation 2 (count word fre- quencies),andthenvector angleisresponsibleforoperation3(computedotproduct).Nextup,we’lltakealookatword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingThe method first calls read file, which returns a list of lines in the input file. We’ll omit the method code, because it is not particularly interesting, and we’ll assume that read file’s running time is proportional to the size of the input file. The input from read line is given to get words from line list, which computes operation 1 (split each document into words).Afterthat,count frequencyturnsthelistofwordsintoadocumentvector(operation 2).def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list = word_list + words_in_line return word_listdef get_words_from_string(line):
 91011121314151617181920212223elif len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word) character_list = []if len(character_list)>0:word = "".join(character_list) word = word.lower() word_list.append(word)return word_list1 2 3 4 5 6 7 8 9101 2 3get words from string takes one line in the input file and breaks it up into a list of words. We iterate through the for loop O(k) times where k is the length of the line. All of the operations within the loop take constant time to run, with the exception of join. Although the join operator takes O(c) time on an input of c characters, each character in the line is joined only once.Therefore,get words from stringhasanoverallruntimeofO(k).get words from line list calls get words from string for each line andcombines the lists into one big list. Line 5 looks innocent but is a big performance killer, becauseusing + to combine W lists of length k is O(W2 ). kkThe output of get words from line list is a list of words, like [’a’, ’cat’,’in’, ’a’, ’bag’].word frequencies from filepassesthisoutputtocount frequency, which turns it into a document vector that counts the number of occurrences of each word, and lookslike[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]].def count_frequency(word_list): L=[]for new_word in word_list: for entry in L:if new_word == entry[0]: entry[1] = entry[1] + 1 breakelse: L.append([new_word,1])return LThe implementation above builds the document vector by takes each word in the input list and looking it up in the list representing the under-construction document vector. In the worst case of a document with all different words, this takes O(W 2 × l) time, where W is the number of words in the document, and l is the average word length.count frequency is the last function call in word frequencies for file. Next up,maincallsvector angle,whichperformsoperation3,computingthedistancemetric.def vector_angle(L1,L2):numerator = inner_product(L1,L2)denominator = math.sqrt(inner_product(L1,L1)*inner_product(L2,L2))6.006 Intro to Algorithmsword_list = [] character_list = [] for c in line:if c.isalnum(): character_list.append(c)Recitation 2September 11, 2013
41 2 3 4 5 6 76.006 Intro to Algorithms Recitation 2 September 11, 2013return math.acos(numerator/denominator)The method is a somewhat straightforward implementation of the distance metric􏰀 L1 · L2 􏰁 􏰄 L1 · L2 􏰅 arccos |L1||L2| = arccos 􏰇(L1 · L1)(L2 · L2),anddelegatestoinner productforthehardworkofcomputingcrossproducts.def inner_product(L1,L2): sum = 0.0for word1, count1 in L1:for word2, count2 in L2: if word1 == word2:sum += count1 * count2 return suminner product is a straightforward inner-product implementation that checks each each word in the first list against the entire second list. The nested loops at lines 3 and 4 give the algorithm its running time of Θ(L1L2), where L1 and L2 are the lengths of the documents’ vectors (the number of unique words in each document).docdist1 Performance ScorecardWe assume that k (number of words per line) is a constant, because the documents will need to fit on screens or paper sheets with a finite length. W (the number of words in a document) is ≥ L (the number of unique words in a document). L21 + L2 + L1L2 = O(L21 + L2) because L21 + L2 ≥ L1L2. Proof (assuming L1, L2 ≥ 0):MethodTimeget words from line list count frequencyword frequencies for fileO(W2 ) = O(W2) kO(WL) O(W2)inner productvector angleO(L1L2) O(L1L2 + L21 + L2) = O(L21 + L2)mainO ( W 12 + W 2 2 )docdist2(L1−L2)2 ≥0 L21+L2−2L1L2 ≥ 0L21+L2 ≥ 2L1L2 L 21 + L 2 2 ≥ L 1 L 2The document distance code invokes the Python profiler to identify the code that takes up the most CPU time. This ensures that we get the biggest returns on our optimization efforts.
1 2 36.006 Intro to Algorithms Recitation 2 September 11, 2013if __name__ == "__main__": import cProfile cProfile.run("main()")You can profile existing programs without changing them by adding -m cProfile -s Time to Python’s command line. For example, the command below will run and profile program.py.  python -m cProfile -s time program.pyA sample profiler output is shown below.123456 ncalls tottime 7 1 0.001 8 2 0.000 9 3 6.25510 1 0.000 11 1 0.004 12 2 0.000 13 2 16.995 14 22663 0.793 15 2 11.205 16 325087 0.034 17 1 0.000 18 1 0.000 19 1241849 0.145 20 1 0.000 21 1300248 0.112 22 232140 0.054 23 232140 0.035 24 2 0.003 25 2 0.000The distance between the documents is: 0.574160 (radians) 3354148 function calls in 35.636 CPU secondsOrdered by: standard namepercall  cumtime  0.001   35.636  0.000   29.376  2.085    6.255  0.000    6.255  0.004   35.635  0.000    0.003  8.498   18.167  0.000    1.172  5.602   11.206  0.000    0.034  0.000    0.000  0.000    0.000  0.000    0.145  0.000    0.000  0.000    0.112  0.000    0.0540.000 0.0350.001 0.0030.000 0.000percall filename:lineno(function) 35.636 <string>:1(<module>) 14.688 docdist1.py:100(word_frequencies_for_file)  2.085 docdist1.py:110(inner_product)  6.255 docdist1.py:125(vector_angle) 35.635 docdist1.py:135(main)  0.001 docdist1.py:37(read_file)  9.083 docdist1.py:49(get_words_from_line_list)  0.000 docdist1.py:61(get_words_from_string)  5.603 docdist1.py:86(count_frequency)  0.000 {len}  0.000 {math.acos}  0.000 {math.sqrt}  0.000 {method ’append’ of ’list’ objects}  0.000 {method ’disable’ of ’_lsprof.Profiler’ objec  0.000 {method ’isalnum’ of ’str’ objects}  0.000 {method ’join’ of ’str’ objects}  0.000 {method ’lower’ of ’str’ objects}  0.001 {method ’readlines’ of ’file’ objects}  0.000 {open}1 2 3 4 5 6Theprofileroutputfordocdist1showsthatthebiggesttimedrainisget words from line list. The problem is that when the + operator is used to concatenate lists, it needs to create a new listand copy the elements of both its operands. Replacing + with extend yields a 30% runtime improvement.def get_words_from_line_list(L): word_list = []for line in L:    words_in_line = get_words_from_string(line)word_list.extend(words_in_line) return word_listextend adds all the elements of an m-element list to an n-element list in Θ(1+m), as opposed to +, which needs to create a new list, and therefore takes Θ(1 + n + m) time. So concatenatingkW lists of k elements takes 􏰆W k = Θ(W) time.k
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist2 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(WL) O(WL)inner productvector angleO(L1L2) O ( L 21 + L 2 2 )mainO(W1L1 + W2L2)1 2 3 4 5 61 2 3 4 5 6 7 8 91 2 3 4 5 6 7docdist3Profilingdocdist2pointstocount frequencyandinner productasgoodtargetsforop- timizations. We’ll speed up inner product by switching to a fast algorithm. However, the algorithmassumesthatthewordsinthedocumentvectorsaresorted.Forexample,[[’a’, 2], [’cat’, 1], [’in’, 1], [’bag’, 1]]needstobecome[[’a’, 2], [’bag’, 1], [’cat’, 1], [’in’, 1]].Firstoff,weaddasteptoword frequencies for filethatsortsthedocumentvector producedbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) insertion_sort(freq_mapping)return freq_mappingThenweimplementinsertion sortusingthealgorithminthetextbook.def insertion_sort(A): for j in range(len(A)):key = A[j]i = j-1while i>-1 and A[i]>key:A[i+1] = A[i]      i = i-1    A[i+1] = keyreturn AInsertion sort runs in O(L2) time, where L is the length of the document vector to be sorted.Finally,inner productisre-implementedusingasimilaralgorithmtothemergingstepin Merge Sort.def inner_product(L1,L2): sum = 0.0i=0j=0while i<len(L1) and j<len(L2):# L1[i:] and L2[j:] yet to be processed if L1[i][0] == L2[j][0]:
8 91011121314151617186.006 Intro to Algorithms Recitation 2 September 11, 2013      # both vectors have this word      sum += L1[i][1] * L2[j][1]      i += 1      j += 1elif L1[i][0] < L2[j][0]:# word L1[i][0] is in L1 but not L2 i += 1else:# word L2[j][0] is in L2 but not L1 j += 1return sumThe new implementation runs in Θ(L1 + L2), where L1 and L2 are the lengths of the two document vectors. We observe that the running time for inner product (and therefore for vector angle)isasymptoticallyoptimal,becauseanyalgorithmthatcomputestheinnerprod- uct will have to read the two vectors, and reading will take Ω(L1 + L2) time.docdist3 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(WL)O(L2) O(WL + L2) = O(WL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1L1 + W2L2)1 2 3 4 5 6 7 8docdist4The next iteration addresses count frequency, which is the biggest time consumer at the moment.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return D.items()The new implementation uses Python dictionaries. The dictionaries are implemented using hash tables, which will be presented in a future lecture. The salient feature of hash tables is that inserting an element using dictionary[key] = value and looking up an element using dictionary[key] both run in O(1) expected time.Instead of storing the document vector under construction in a list, the new implementation uses a dictionary. The keys are the words in the document, and the value are the number of times
6.006 Intro to Algorithms Recitation 2 September 11, 2013 each word appears in the document. Since both insertion (line 5) and lookup (line 7) take O(1)time, building a document vector out of W words takes O(W ) time. docdist4 Performance ScorecardMethodTimeget words from line list count frequencyinsertion sortword frequencies for fileO(W) O(W)O(L2) O(W + L2) = O(L2)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L21 +L2)1 2 3 4 5 6 7docdist5Thisiterationsimplifiesget words from stringthatbreaksuplinesintowords.Firstoff, the standard library function string.translate is used for converting uppercase characters to lowercase, and for converting punctuation to spaces. Second, the split method on strings is used to break up a line into words.translation_table = string.maketrans(string.punctuation+string.uppercase,                   " "*len(string.punctuation)+string.lowercase)def get_words_from_string(line):line = line.translate(translation_table) word_list = line.split()return word_listThe main benefit of this change is that 23 lines of code are replaced with 5 lines of code. This makes the implementation easier to analyze. A side benefit is that many functions in the Python standard library are implemented in directly in C (a low-level programming language that is very close to machine code), which gives them better performance. Although the running time is asymptotically the same, the hidden constants are much better for the C code than for our Python code presented in docdist1.docdist5 Performance ScorecardIdentical to the docdist4 scorecard.docdist6Nowthatallthedistractionsareoutoftheway,it’stimetotackleinsertion sort,whichis takes up the most CPU time, by far, in the profiler output for docdist5.The advantages of insertion sort are that it sorts in place, and it is simple to implement. However, its worst-case running time is O(N2) for an N-element array. We’ll replace insertion
1 2 3 4 5 61 2 3 4 5 6 7 8 9101112131415161718192021222324256.006 Intro to Algorithms Recitation 2 September 11, 2013 sort with a better algorithm, merge sort. Merge sort is not in place, so we’ll need to modifyword frequencies for file.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) freq_mapping = merge_sort(freq_mapping)return freq_mappingThemerge sortimplementationcloselyfollowsthepseudocodeinthetextbook.def merge_sort(A): n = len(A)if n==1:return Amid = n//2L = merge_sort(A[:mid]) R = merge_sort(A[mid:]) return merge(L,R)def merge(L,R): i=0j=0answer = []while i<len(L) and j<len(R):if L[i]<R[j]: answer.append(L[i]) i += 1else: answer.append(R[j]) j += 1if i<len(L): answer.extend(L[i:])if j<len(R): answer.extend(R[j:])return answerThe textbook proves that merge sort runs in Θ(n log n) time. You should apply your knowledge of the Python cost model to convince yourself that the implementation above also runs in Θ(n log n) time.
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist6 Performance ScorecardMethodTimeget words from line list count frequencymerge sortword frequencies for fileO(W)O(W) O(L log L) O(W +LlogL)=O(LlogL)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 +W2 +L1 logL1 +L2 logL2)1 2 3 4 5 6 7 81 2 3 4 51 2 3 4 5 6docdist7Switching to merge sort improved the running time dramatically. However, if we look at docdist6’s profiler output, we notice that merge is the function with the biggest runtime footprint. Merge sort’s performance in practice is great, so it seems that the only way to make our code faster is to get rid of sorting altogether.This iteration switches away from the sorted list representation of document vectors, and in- steadusesthePythondictionaryrepresentationthatwasintroducedindocdist4.count frequency already used that representation internally, so we only need to remove the code that converted the Python dictionary to a list.def count_frequency(word_list): D={}for new_word in word_list: if new_word in D:D[new_word] = D[new_word]+1 else:D[new_word] = 1 return DThis method still takes O(W ) time to process a W -word document.word frequencies for file does not call merge sort anymore, and instead re- turnsthedictionarybuiltbycount frequency.def word_frequencies_for_file(filename): line_list = read_file(filename)word_list = get_words_from_line_list(line_list) freq_mapping = count_frequency(word_list) return freq_mappingNextup,inner productmakesusesdictionarylookupsinsteadofmergingsortedlists.def inner_product(D1,D2): sum = 0.0for key in D1:if key in D2:sum += D1[key] * D2[key]return sum
6.006 Intro to Algorithms Recitation 2 September 11, 2013The logic is quite similar to the straightforward inner product in docdist1. Each word in the first document vector is looked up in the second document vector. However, because the document vectors are dictionaries, each takes O(1) time, and inner product runs in O(L1) time, where L1 is the length of the first document’s vector.docdist7 Performance ScorecardMethodTimeget words from line list count frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)1 2 3 4 5 6 7 8 910docdist8At this point, all the algorithms in our solution are asymptotically optimal. We can easily prove this, by noting that each of the 3 main operations runs in time proportional to its input size, and each operation needs to read all its input to produce its output. However, there is still room for optimizing and simplifying the code.There is no reason to read each document line by line, and then break up each line into words. The last iteration processes reads each document into one large string, and breaks up the entire document into words at once.Firstoff,read fileismodifiedtoreturnasinglestring,insteadofanarrayofstrings.Then,get words from line listisrenamedtoget words from file,andissimplified, becauseitdoesn’tneedtoiterateoveralistoflinesanymore.Last,word frequencies for file is updated to reflect the renaming.def get_words_from_string(string):string = string.translate(translation_table) word_list = string.split()return word_listdef word_frequencies_for_file(filename): text = read_file(filename)word_list = get_words_from_string(text) freq_mapping = count_frequency(word_list) return freq_mapping
6.006 Intro to Algorithms Recitation 2 September 11, 2013docdist8 Performance ScorecardMethodTimeget words from textcount frequencyword frequencies for fileO(W) O(W) O(W)inner productvector angleO(L1 + L2) O(L1 + L2)mainO(W1 + W2)
Last login: Wed Jan 22 17:19:34 on ttys000
SimonMacbookAir:~ simon_lui$ python
Enthought Python Distribution (EPD) free version -- www.enthought.com
Version: 7.3-2 (32-bit)
(type 'upgrade' or see www.enthought.com/epd/upgrade to get the full EPD)

Python 2.7.3 |EPD_free 7.3-2 (32-bit)| (default, Apr 12 2012, 11:28:34) 
[GCC 4.0.1 (Apple Inc. build 5493)] on darwin
Type "credits", "demo" or "enthought" for more information.
>>> ls
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'ls' is not defined
>>> 
  [Restored]
Last login: Fri Jan 24 12:42:18 on ttys000
SimonMacbookAir:~ simon_lui$ ls
AndroidStudioProjects	Library			Public
Desktop			ML_repo			Strongsync
Documents		Movies			Virtual Machine
Downloads		Music
Dropbox			Pictures
SimonMacbookAir:~ simon_lui$ cd "/Users/nomislui/Dropbox/DocumentDB"
SimonMacbookAir:DocumentDB simon_lui$ cd todo
SimonMacbookAir:todo simon_lui$ ls
20140123_212917_resized.jpg	ladan
How To Add JUnit.docx		lec2 code
docdist1.py			prog1.py
file1.txt			rec02_code
file2.txt
SimonMacbookAir:todo simon_lui$ docdist1.py file1 file2
-bash: docdist1.py: command not found
SimonMacbookAir:todo simon_lui$ docdist1.py
-bash: docdist1.py: command not found
SimonMacbookAir:todo simon_lui$ python docdist1.py
Usage: docdist1.py filename_1 filename_2
         4 function calls in 0.000 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        1    0.000    0.000    0.000    0.000 docdist1.py:135(main)
        1    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}


SimonMacbookAir:todo simon_lui$ python docdist1.py file1 file12
Error opening or reading input file:  file1
         8 function calls in 0.000 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        1    0.000    0.000    0.000    0.000 docdist1.py:100(word_frequencies_for_file)
        1    0.000    0.000    0.000    0.000 docdist1.py:135(main)
        1    0.000    0.000    0.000    0.000 docdist1.py:37(read_file)
        1    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {open}
        1    0.000    0.000    0.000    0.000 {sys.exit}


SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ python docdist1.py file1 file2
Error opening or reading input file:  file1
         8 function calls in 0.000 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        1    0.000    0.000    0.000    0.000 docdist1.py:100(word_frequencies_for_file)
        1    0.000    0.000    0.000    0.000 docdist1.py:135(main)
        1    0.000    0.000    0.000    0.000 docdist1.py:37(read_file)
        1    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {open}
        1    0.000    0.000    0.000    0.000 {sys.exit}


SimonMacbookAir:todo simon_lui$ python docdist1.py file1 file21
Error opening or reading input file:  file1
         8 function calls in 0.000 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        1    0.000    0.000    0.000    0.000 docdist1.py:100(word_frequencies_for_file)
        1    0.000    0.000    0.000    0.000 docdist1.py:135(main)
        1    0.000    0.000    0.000    0.000 docdist1.py:37(read_file)
        1    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {open}
        1    0.000    0.000    0.000    0.000 {sys.exit}


SimonMacbookAir:todo simon_lui$ python docdist1.py
Usage: docdist1.py filename_1 filename_2
         4 function calls in 0.000 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        1    0.000    0.000    0.000    0.000 docdist1.py:135(main)
        1    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}


SimonMacbookAir:todo simon_lui$ python docdist1.py
Usage: docdist1.py filename_1 filename_2
         4 function calls in 0.000 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        1    0.000    0.000    0.000    0.000 docdist1.py:135(main)
        1    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}


SimonMacbookAir:todo simon_lui$ python docdist1.py file1.txt files2.txt
Error opening or reading input file:  files2.txt
         83 function calls in 0.000 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        2    0.000    0.000    0.000    0.000 docdist1.py:100(word_frequencies_for_file)
        1    0.000    0.000    0.000    0.000 docdist1.py:135(main)
        2    0.000    0.000    0.000    0.000 docdist1.py:37(read_file)
        1    0.000    0.000    0.000    0.000 docdist1.py:49(get_words_from_line_list)
        1    0.000    0.000    0.000    0.000 docdist1.py:61(get_words_from_string)
        1    0.000    0.000    0.000    0.000 docdist1.py:86(count_frequency)
        6    0.000    0.000    0.000    0.000 {len}
       29    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
       24    0.000    0.000    0.000    0.000 {method 'isalnum' of 'str' objects}
        5    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
        5    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
        1    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {open}
        1    0.000    0.000    0.000    0.000 {sys.exit}


SimonMacbookAir:todo simon_lui$ python docdist1.py file1.txt file2.txt
The distance between the documents is: 0.387597 (radians)
         146 function calls in 0.002 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.002    0.002 <string>:1(<module>)
        2    0.000    0.000    0.000    0.000 docdist1.py:100(word_frequencies_for_file)
        3    0.000    0.000    0.000    0.000 docdist1.py:110(inner_product)
        1    0.000    0.000    0.002    0.002 docdist1.py:125(vector_angle)
        1    0.000    0.000    0.002    0.002 docdist1.py:135(main)
        2    0.000    0.000    0.000    0.000 docdist1.py:37(read_file)
        2    0.000    0.000    0.000    0.000 docdist1.py:49(get_words_from_line_list)
        2    0.000    0.000    0.000    0.000 docdist1.py:61(get_words_from_string)
        2    0.000    0.000    0.000    0.000 docdist1.py:86(count_frequency)
       10    0.000    0.000    0.000    0.000 {len}
        1    0.002    0.002    0.002    0.002 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
       52    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
       43    0.000    0.000    0.000    0.000 {method 'isalnum' of 'str' objects}
        9    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
        9    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ python docdist1.py file1.txt file2.txt
The distance between the documents is: 0.000000 (radians)
         160 function calls in 0.000 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        2    0.000    0.000    0.000    0.000 docdist1.py:100(word_frequencies_for_file)
        3    0.000    0.000    0.000    0.000 docdist1.py:110(inner_product)
        1    0.000    0.000    0.000    0.000 docdist1.py:125(vector_angle)
        1    0.000    0.000    0.000    0.000 docdist1.py:135(main)
        2    0.000    0.000    0.000    0.000 docdist1.py:37(read_file)
        2    0.000    0.000    0.000    0.000 docdist1.py:49(get_words_from_line_list)
        2    0.000    0.000    0.000    0.000 docdist1.py:61(get_words_from_string)
        2    0.000    0.000    0.000    0.000 docdist1.py:86(count_frequency)
       11    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
       58    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
       48    0.000    0.000    0.000    0.000 {method 'isalnum' of 'str' objects}
       10    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
       10    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ python docdist1.py file1.txt file2.txt
The distance between the documents is: 0.000000 (radians)
         57 function calls in 0.001 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.001    0.001 <string>:1(<module>)
        2    0.000    0.000    0.000    0.000 docdist1.py:100(word_frequencies_for_file)
        3    0.000    0.000    0.000    0.000 docdist1.py:110(inner_product)
        1    0.000    0.000    0.000    0.000 docdist1.py:125(vector_angle)
        1    0.000    0.000    0.001    0.001 docdist1.py:135(main)
        2    0.000    0.000    0.000    0.000 docdist1.py:37(read_file)
        2    0.000    0.000    0.000    0.000 docdist1.py:49(get_words_from_line_list)
        2    0.000    0.000    0.000    0.000 docdist1.py:61(get_words_from_string)
        2    0.000    0.000    0.000    0.000 docdist1.py:86(count_frequency)
        4    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
       14    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
       10    0.000    0.000    0.000    0.000 {method 'isalnum' of 'str' objects}
        3    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
        3    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ python docdist1.py file1.txt file2.txt
The distance between the documents is: 0.000000 (radians)
         57 function calls in 0.000 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        2    0.000    0.000    0.000    0.000 docdist1.py:100(word_frequencies_for_file)
        3    0.000    0.000    0.000    0.000 docdist1.py:110(inner_product)
        1    0.000    0.000    0.000    0.000 docdist1.py:125(vector_angle)
        1    0.000    0.000    0.000    0.000 docdist1.py:135(main)
        2    0.000    0.000    0.000    0.000 docdist1.py:37(read_file)
        2    0.000    0.000    0.000    0.000 docdist1.py:49(get_words_from_line_list)
        2    0.000    0.000    0.000    0.000 docdist1.py:61(get_words_from_string)
        2    0.000    0.000    0.000    0.000 docdist1.py:86(count_frequency)
        4    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
       14    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
       10    0.000    0.000    0.000    0.000 {method 'isalnum' of 'str' objects}
        3    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
        3    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ python docdist1.py file1.txt file2.txt
The distance between the documents is: 1.150262 (radians)
         98 function calls in 0.000 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        2    0.000    0.000    0.000    0.000 docdist1.py:100(word_frequencies_for_file)
        3    0.000    0.000    0.000    0.000 docdist1.py:110(inner_product)
        1    0.000    0.000    0.000    0.000 docdist1.py:125(vector_angle)
        1    0.000    0.000    0.000    0.000 docdist1.py:135(main)
        2    0.000    0.000    0.000    0.000 docdist1.py:37(read_file)
        2    0.000    0.000    0.000    0.000 docdist1.py:49(get_words_from_line_list)
        2    0.000    0.000    0.000    0.000 docdist1.py:61(get_words_from_string)
        2    0.000    0.000    0.000    0.000 docdist1.py:86(count_frequency)
        8    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
       30    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
       25    0.000    0.000    0.000    0.000 {method 'isalnum' of 'str' objects}
        6    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
        6    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ python docdist1.py file1.txt file2.txt
The distance between the documents is: 0.755752 (radians)
         41183 function calls in 0.098 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.098    0.098 <string>:1(<module>)
        2    0.000    0.000    0.064    0.032 docdist1.py:100(word_frequencies_for_file)
        3    0.034    0.011    0.034    0.011 docdist1.py:110(inner_product)
        1    0.000    0.000    0.034    0.034 docdist1.py:125(vector_angle)
        1    0.000    0.000    0.098    0.098 docdist1.py:135(main)
        2    0.000    0.000    0.000    0.000 docdist1.py:37(read_file)
        2    0.002    0.001    0.027    0.014 docdist1.py:49(get_words_from_line_list)
      310    0.017    0.000    0.025    0.000 docdist1.py:61(get_words_from_string)
        2    0.036    0.018    0.036    0.018 docdist1.py:86(count_frequency)
     3790    0.001    0.000    0.001    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
    15717    0.003    0.000    0.003    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
    15717    0.003    0.000    0.003    0.000 {method 'isalnum' of 'str' objects}
     2814    0.001    0.000    0.001    0.000 {method 'join' of 'str' objects}
     2814    0.001    0.000    0.001    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ python docdist1.py file1.txt file2.txt
The distance between the documents is: 0.755752 (radians)
         41183 function calls in 0.107 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.107    0.107 <string>:1(<module>)
        2    0.000    0.000    0.068    0.034 docdist1.py:100(word_frequencies_for_file)
        3    0.039    0.013    0.039    0.013 docdist1.py:110(inner_product)
        1    0.000    0.000    0.039    0.039 docdist1.py:125(vector_angle)
        1    0.000    0.000    0.107    0.107 docdist1.py:135(main)
        2    0.000    0.000    0.000    0.000 docdist1.py:37(read_file)
        2    0.002    0.001    0.027    0.013 docdist1.py:49(get_words_from_line_list)
      310    0.017    0.000    0.025    0.000 docdist1.py:61(get_words_from_string)
        2    0.041    0.020    0.041    0.020 docdist1.py:86(count_frequency)
     3790    0.001    0.000    0.001    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
    15717    0.003    0.000    0.003    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
    15717    0.003    0.000    0.003    0.000 {method 'isalnum' of 'str' objects}
     2814    0.001    0.000    0.001    0.000 {method 'join' of 'str' objects}
     2814    0.001    0.000    0.001    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ python docdist2.py file1.txt file2.txt
The distance between the documents is: 0.755752 (radians)
         41493 function calls in 0.091 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.091    0.091 <string>:1(<module>)
        2    0.000    0.000    0.057    0.028 docdist2.py:101(word_frequencies_for_file)
        3    0.034    0.011    0.034    0.011 docdist2.py:111(inner_product)
        1    0.000    0.000    0.034    0.034 docdist2.py:126(vector_angle)
        1    0.000    0.000    0.091    0.091 docdist2.py:136(main)
        2    0.000    0.000    0.000    0.000 docdist2.py:37(read_file)
        2    0.000    0.000    0.022    0.011 docdist2.py:49(get_words_from_line_list)
      310    0.015    0.000    0.022    0.000 docdist2.py:62(get_words_from_string)
        2    0.034    0.017    0.034    0.017 docdist2.py:87(count_frequency)
     3790    0.001    0.000    0.001    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
    15717    0.003    0.000    0.003    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      310    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}
    15717    0.002    0.000    0.002    0.000 {method 'isalnum' of 'str' objects}
     2814    0.001    0.000    0.001    0.000 {method 'join' of 'str' objects}
     2814    0.001    0.000    0.001    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ python docdist2.py file1.txt file2.txt
The distance between the documents is: 0.755752 (radians)
         41493 function calls in 0.091 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.091    0.091 <string>:1(<module>)
        2    0.000    0.000    0.055    0.027 docdist2.py:101(word_frequencies_for_file)
        3    0.036    0.012    0.036    0.012 docdist2.py:111(inner_product)
        1    0.000    0.000    0.036    0.036 docdist2.py:126(vector_angle)
        1    0.000    0.000    0.091    0.091 docdist2.py:136(main)
        2    0.000    0.000    0.000    0.000 docdist2.py:37(read_file)
        2    0.000    0.000    0.022    0.011 docdist2.py:49(get_words_from_line_list)
      310    0.014    0.000    0.021    0.000 docdist2.py:62(get_words_from_string)
        2    0.033    0.016    0.033    0.016 docdist2.py:87(count_frequency)
     3790    0.001    0.000    0.001    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
    15717    0.002    0.000    0.002    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      310    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}
    15717    0.002    0.000    0.002    0.000 {method 'isalnum' of 'str' objects}
     2814    0.001    0.000    0.001    0.000 {method 'join' of 'str' objects}
     2814    0.001    0.000    0.001    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ python docdist2.py file1.txt file2.txt
The distance between the documents is: 0.755752 (radians)
         41493 function calls in 0.091 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.091    0.091 <string>:1(<module>)
        2    0.000    0.000    0.055    0.027 docdist2.py:101(word_frequencies_for_file)
        3    0.036    0.012    0.036    0.012 docdist2.py:111(inner_product)
        1    0.000    0.000    0.036    0.036 docdist2.py:126(vector_angle)
        1    0.000    0.000    0.091    0.091 docdist2.py:136(main)
        2    0.000    0.000    0.000    0.000 docdist2.py:37(read_file)
        2    0.000    0.000    0.022    0.011 docdist2.py:49(get_words_from_line_list)
      310    0.015    0.000    0.021    0.000 docdist2.py:62(get_words_from_string)
        2    0.033    0.016    0.033    0.017 docdist2.py:87(count_frequency)
     3790    0.001    0.000    0.001    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
    15717    0.002    0.000    0.002    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      310    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}
    15717    0.002    0.000    0.002    0.000 {method 'isalnum' of 'str' objects}
     2814    0.001    0.000    0.001    0.000 {method 'join' of 'str' objects}
     2814    0.001    0.000    0.001    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ python docdist3.py file1.txt file2.txt
The distance between the documents is: 0.755752 (radians)
         43924 function calls in 0.084 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.084    0.084 <string>:1(<module>)
        2    0.025    0.013    0.025    0.013 docdist3.py:102(insertion_sort)
        2    0.000    0.000    0.082    0.041 docdist3.py:121(word_frequencies_for_file)
        3    0.002    0.001    0.002    0.001 docdist3.py:132(inner_product)
        1    0.000    0.000    0.002    0.002 docdist3.py:158(vector_angle)
        1    0.000    0.000    0.084    0.084 docdist3.py:168(main)
        2    0.000    0.000    0.000    0.000 docdist3.py:38(read_file)
        2    0.000    0.000    0.022    0.011 docdist3.py:50(get_words_from_line_list)
      310    0.015    0.000    0.022    0.000 docdist3.py:63(get_words_from_string)
        2    0.034    0.017    0.034    0.017 docdist3.py:88(count_frequency)
     6217    0.001    0.000    0.001    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
    15717    0.003    0.000    0.003    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      310    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}
    15717    0.002    0.000    0.002    0.000 {method 'isalnum' of 'str' objects}
     2814    0.001    0.000    0.001    0.000 {method 'join' of 'str' objects}
     2814    0.001    0.000    0.001    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {open}
        2    0.000    0.000    0.000    0.000 {range}


SimonMacbookAir:todo simon_lui$ python docdist8.py file1.txt file2.txt
The distance between the documents is: 0.755752 (radians)
         26 function calls in 0.002 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.002    0.002 <string>:1(<module>)
        1    0.000    0.000    0.000    0.000 docdist8.py:100(vector_angle)
        1    0.000    0.000    0.002    0.002 docdist8.py:110(main)
        2    0.000    0.000    0.000    0.000 docdist8.py:39(read_file)
        2    0.000    0.000    0.000    0.000 docdist8.py:56(get_words_from_text)
        2    0.001    0.000    0.001    0.000 docdist8.py:65(count_frequency)
        2    0.000    0.000    0.001    0.001 docdist8.py:77(word_frequencies_for_file)
        3    0.000    0.000    0.000    0.000 docdist8.py:86(inner_product)
        1    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        2    0.000    0.000    0.000    0.000 {method 'read' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'translate' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ python docdist7.py file1.txt file2.txt
The distance between the documents is: 0.755752 (radians)
         1262 function calls in 0.003 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.003    0.003 <string>:1(<module>)
        3    0.000    0.000    0.000    0.000 docdist7.py:103(inner_product)
        1    0.000    0.000    0.000    0.000 docdist7.py:117(vector_angle)
        1    0.000    0.000    0.003    0.003 docdist7.py:127(main)
        2    0.000    0.000    0.000    0.000 docdist7.py:39(read_file)
        2    0.000    0.000    0.001    0.001 docdist7.py:51(get_words_from_line_list)
      310    0.000    0.000    0.001    0.000 docdist7.py:68(get_words_from_string)
        2    0.001    0.000    0.001    0.000 docdist7.py:81(count_frequency)
        2    0.000    0.000    0.002    0.001 docdist7.py:93(word_frequencies_for_file)
        1    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      310    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
      310    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}
      310    0.000    0.000    0.000    0.000 {method 'translate' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ python docdist7.py file1.txt file2.txt
The distance between the documents is: 0.755752 (radians)
         1262 function calls in 0.005 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.005    0.005 <string>:1(<module>)
        3    0.000    0.000    0.000    0.000 docdist7.py:103(inner_product)
        1    0.000    0.000    0.000    0.000 docdist7.py:117(vector_angle)
        1    0.000    0.000    0.004    0.004 docdist7.py:127(main)
        2    0.000    0.000    0.001    0.001 docdist7.py:39(read_file)
        2    0.000    0.000    0.001    0.001 docdist7.py:51(get_words_from_line_list)
      310    0.000    0.000    0.001    0.000 docdist7.py:68(get_words_from_string)
        2    0.001    0.001    0.001    0.001 docdist7.py:81(count_frequency)
        2    0.000    0.000    0.004    0.002 docdist7.py:93(word_frequencies_for_file)
        1    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      310    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}
        2    0.001    0.001    0.001    0.001 {method 'readlines' of 'file' objects}
      310    0.001    0.000    0.001    0.000 {method 'split' of 'str' objects}
      310    0.000    0.000    0.000    0.000 {method 'translate' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ python docdist1.py file1.txt file2.txt
The distance between the documents is: 0.755752 (radians)
         41183 function calls in 0.094 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.094    0.094 <string>:1(<module>)
        2    0.000    0.000    0.057    0.028 docdist1.py:100(word_frequencies_for_file)
        3    0.037    0.012    0.037    0.012 docdist1.py:110(inner_product)
        1    0.000    0.000    0.038    0.038 docdist1.py:125(vector_angle)
        1    0.000    0.000    0.094    0.094 docdist1.py:135(main)
        2    0.000    0.000    0.000    0.000 docdist1.py:37(read_file)
        2    0.002    0.001    0.023    0.012 docdist1.py:49(get_words_from_line_list)
      310    0.015    0.000    0.022    0.000 docdist1.py:61(get_words_from_string)
        2    0.033    0.016    0.033    0.017 docdist1.py:86(count_frequency)
     3790    0.001    0.000    0.001    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
    15717    0.003    0.000    0.003    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
    15717    0.002    0.000    0.002    0.000 {method 'isalnum' of 'str' objects}
     2814    0.001    0.000    0.001    0.000 {method 'join' of 'str' objects}
     2814    0.001    0.000    0.001    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ python docdist1.py file1.txt file2.txt
The distance between the documents is: 0.785398 (radians)
         136 function calls in 0.000 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        2    0.000    0.000    0.000    0.000 docdist1.py:100(word_frequencies_for_file)
        3    0.000    0.000    0.000    0.000 docdist1.py:110(inner_product)
        1    0.000    0.000    0.000    0.000 docdist1.py:125(vector_angle)
        1    0.000    0.000    0.000    0.000 docdist1.py:135(main)
        2    0.000    0.000    0.000    0.000 docdist1.py:37(read_file)
        2    0.000    0.000    0.000    0.000 docdist1.py:49(get_words_from_line_list)
        2    0.000    0.000    0.000    0.000 docdist1.py:61(get_words_from_string)
        2    0.000    0.000    0.000    0.000 docdist1.py:86(count_frequency)
       12    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
       45    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
       34    0.000    0.000    0.000    0.000 {method 'isalnum' of 'str' objects}
       11    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
       11    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ python docdist7.py file1.txt file2.txt
The distance between the documents is: 0.785398 (radians)
         30 function calls in 0.000 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        3    0.000    0.000    0.000    0.000 docdist7.py:103(inner_product)
        1    0.000    0.000    0.000    0.000 docdist7.py:117(vector_angle)
        1    0.000    0.000    0.000    0.000 docdist7.py:127(main)
        2    0.000    0.000    0.000    0.000 docdist7.py:39(read_file)
        2    0.000    0.000    0.000    0.000 docdist7.py:51(get_words_from_line_list)
        2    0.000    0.000    0.000    0.000 docdist7.py:68(get_words_from_string)
        2    0.000    0.000    0.000    0.000 docdist7.py:81(count_frequency)
        2    0.000    0.000    0.000    0.000 docdist7.py:93(word_frequencies_for_file)
        1    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        2    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'translate' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ python docdist1.py file1.txt file2.txt
  File "docdist1.py", line 96
    else:
       ^
SyntaxError: invalid syntax
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ python docdist1.py file1.txt file2.txt
The distance between the documents is: 0.785398 (radians)
         136 function calls in 0.000 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        2    0.000    0.000    0.000    0.000 docdist1.py:100(word_frequencies_for_file)
        3    0.000    0.000    0.000    0.000 docdist1.py:110(inner_product)
        1    0.000    0.000    0.000    0.000 docdist1.py:125(vector_angle)
        1    0.000    0.000    0.000    0.000 docdist1.py:135(main)
        2    0.000    0.000    0.000    0.000 docdist1.py:37(read_file)
        2    0.000    0.000    0.000    0.000 docdist1.py:49(get_words_from_line_list)
        2    0.000    0.000    0.000    0.000 docdist1.py:61(get_words_from_string)
        2    0.000    0.000    0.000    0.000 docdist1.py:86(count_frequency)
       12    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
       45    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
       34    0.000    0.000    0.000    0.000 {method 'isalnum' of 'str' objects}
       11    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
       11    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ python docdist1.py file1.txt file2.txt
The distance between the documents is: 0.785398 (radians)
         136 function calls in 0.000 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        2    0.000    0.000    0.000    0.000 docdist1.py:100(word_frequencies_for_file)
        3    0.000    0.000    0.000    0.000 docdist1.py:110(inner_product)
        1    0.000    0.000    0.000    0.000 docdist1.py:125(vector_angle)
        1    0.000    0.000    0.000    0.000 docdist1.py:135(main)
        2    0.000    0.000    0.000    0.000 docdist1.py:37(read_file)
        2    0.000    0.000    0.000    0.000 docdist1.py:49(get_words_from_line_list)
        2    0.000    0.000    0.000    0.000 docdist1.py:61(get_words_from_string)
        2    0.000    0.000    0.000    0.000 docdist1.py:86(count_frequency)
       12    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
       45    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
       34    0.000    0.000    0.000    0.000 {method 'isalnum' of 'str' objects}
       11    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
       11    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ python docdist6.py file1.txt file2.txt
The distance between the documents is: 0.785398 (radians)
         173 function calls (159 primitive calls) in 0.000 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 <string>:1(<module>)
        7    0.000    0.000    0.000    0.000 docdist6.py:105(merge)
        2    0.000    0.000    0.000    0.000 docdist6.py:144(word_frequencies_for_file)
        3    0.000    0.000    0.000    0.000 docdist6.py:155(inner_product)
        1    0.000    0.000    0.000    0.000 docdist6.py:181(vector_angle)
        1    0.000    0.000    0.000    0.000 docdist6.py:191(main)
        2    0.000    0.000    0.000    0.000 docdist6.py:39(read_file)
        2    0.000    0.000    0.000    0.000 docdist6.py:51(get_words_from_line_list)
        2    0.000    0.000    0.000    0.000 docdist6.py:68(get_words_from_string)
        2    0.000    0.000    0.000    0.000 docdist6.py:81(count_frequency)
     16/2    0.000    0.000    0.000    0.000 docdist6.py:93(merge_sort)
       99    0.000    0.000    0.000    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
       13    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        9    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}
        2    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
        2    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {method 'translate' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ python docdist6.py file1.txt file2.txt
The distance between the documents is: 0.660944 (radians)
         30226 function calls (28540 primitive calls) in 0.020 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.020    0.020 <string>:1(<module>)
      843    0.009    0.000    0.012    0.000 docdist6.py:105(merge)
        2    0.000    0.000    0.017    0.009 docdist6.py:144(word_frequencies_for_file)
        3    0.002    0.001    0.002    0.001 docdist6.py:155(inner_product)
        1    0.000    0.000    0.002    0.002 docdist6.py:181(vector_angle)
        1    0.000    0.000    0.020    0.020 docdist6.py:191(main)
        2    0.000    0.000    0.000    0.000 docdist6.py:39(read_file)
        2    0.000    0.000    0.001    0.000 docdist6.py:51(get_words_from_line_list)
       18    0.000    0.000    0.001    0.000 docdist6.py:68(get_words_from_string)
        2    0.001    0.001    0.002    0.001 docdist6.py:81(count_frequency)
   1688/2    0.003    0.000    0.015    0.008 docdist6.py:93(merge_sort)
    20344    0.002    0.000    0.002    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
     6413    0.001    0.000    0.001    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      861    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}
        2    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
       18    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}
       18    0.000    0.000    0.000    0.000 {method 'translate' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ python docdist6.py file1.txt file2.txt
The distance between the documents is: 0.660944 (radians)
         30714 function calls (29028 primitive calls) in 0.044 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.044    0.044 <string>:1(<module>)
      843    0.011    0.000    0.016    0.000 docdist6.py:105(merge)
        2    0.000    0.000    0.039    0.020 docdist6.py:144(word_frequencies_for_file)
        3    0.003    0.001    0.003    0.001 docdist6.py:155(inner_product)
        1    0.000    0.000    0.003    0.003 docdist6.py:181(vector_angle)
        1    0.001    0.001    0.044    0.044 docdist6.py:191(main)
        2    0.000    0.000    0.001    0.000 docdist6.py:39(read_file)
        2    0.000    0.000    0.006    0.003 docdist6.py:51(get_words_from_line_list)
      140    0.000    0.000    0.005    0.000 docdist6.py:68(get_words_from_string)
        2    0.014    0.007    0.014    0.007 docdist6.py:81(count_frequency)
   1688/2    0.003    0.000    0.019    0.009 docdist6.py:93(merge_sort)
    20344    0.003    0.000    0.003    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
     6413    0.001    0.000    0.001    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      983    0.001    0.000    0.001    0.000 {method 'extend' of 'list' objects}
        2    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
      140    0.004    0.000    0.004    0.000 {method 'split' of 'str' objects}
      140    0.001    0.000    0.001    0.000 {method 'translate' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ 
SimonMacbookAir:todo simon_lui$ python docdist6.py file1.txt file2.txt
The distance between the documents is: 0.660944 (radians)
         30714 function calls (29028 primitive calls) in 0.038 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.038    0.038 <string>:1(<module>)
      843    0.009    0.000    0.012    0.000 docdist6.py:105(merge)
        2    0.000    0.000    0.035    0.018 docdist6.py:144(word_frequencies_for_file)
        3    0.002    0.001    0.002    0.001 docdist6.py:155(inner_product)
        1    0.000    0.000    0.002    0.002 docdist6.py:181(vector_angle)
        1    0.001    0.001    0.038    0.038 docdist6.py:191(main)
        2    0.000    0.000    0.001    0.000 docdist6.py:39(read_file)
        2    0.000    0.000    0.005    0.003 docdist6.py:51(get_words_from_line_list)
      140    0.000    0.000    0.005    0.000 docdist6.py:68(get_words_from_string)
        2    0.014    0.007    0.014    0.007 docdist6.py:81(count_frequency)
   1688/2    0.003    0.000    0.015    0.008 docdist6.py:93(merge_sort)
    20344    0.003    0.000    0.003    0.000 {len}
        1    0.000    0.000    0.000    0.000 {math.acos}
        1    0.000    0.000    0.000    0.000 {math.sqrt}
     6413    0.001    0.000    0.001    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
      983    0.001    0.000    0.001    0.000 {method 'extend' of 'list' objects}
        2    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}
        2    0.001    0.000    0.001    0.000 {method 'readlines' of 'file' objects}
      140    0.004    0.000    0.004    0.000 {method 'split' of 'str' objects}
      140    0.001    0.000    0.001    0.000 {method 'translate' of 'str' objects}
        2    0.000    0.000    0.000    0.000 {open}


SimonMacbookAir:todo simon_lui$ 
